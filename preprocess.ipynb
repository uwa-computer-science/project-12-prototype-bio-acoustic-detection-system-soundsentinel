{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import soundfile as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths for the audio files and corresponding CSV annotations\n",
    "audio_files = ['../data/Development_set/Development_Set/Training_Set/MT/dcase_MK1.wav', '../data/Development_set/Development_Set/Training_Set/MT/dcase_MK2.wav']\n",
    "csv_files = ['../data/Development_set/Development_Set/Training_Set/MT/dcase_MK1.csv', '../data/Development_set/Development_Set/Training_Set/MT/dcase_MK2.csv']\n",
    "\n",
    "# Output directory for AFE segments\n",
    "output_dir = 'processed_segments'\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from Esther's preprocessing code for audio segmentation from processed_segments.py\n",
    "# Target sampling rate\n",
    "sr = 8000\n",
    "\n",
    "# Upsampling target rate\n",
    "target_sr = 101640\n",
    "\n",
    "# Initialize a dictionary to keep track of the number of segments for each label\n",
    "label_count = {}\n",
    "\n",
    "# Function to upsample audio segments\n",
    "def upsample_audio(segment, initial_sr, target_sr):\n",
    "    duration = len(segment) / initial_sr\n",
    "    initial_time_points = np.linspace(0, duration, len(segment))\n",
    "    target_time_points = np.linspace(0, duration, int(duration * target_sr))\n",
    "    upsampled_segment = np.interp(target_time_points, initial_time_points, segment) # Perform linear interpolation to resample the original audio segment to the new time points\n",
    "    return upsampled_segment\n",
    "\n",
    "# Iterate through each audio file and its corresponding CSV annotation file\n",
    "for audio_file, csv_file in zip(audio_files, csv_files):\n",
    "    # Load the audio file\n",
    "    y, sr = librosa.load(audio_file, sr=sr)\n",
    "\n",
    "    # Load the CSV annotations\n",
    "    annotations = pd.read_csv(csv_file)\n",
    "\n",
    "    # Iterate over each row in the CSV to extract annotation details\n",
    "    for i, annotation in annotations.iterrows():\n",
    "        # Skip the segment if any of the annotation columns contain 'UNK'\n",
    "        if 'UNK' in [annotation['SNMK'], annotation['CCMK'], annotation['AGGM'], annotation['SOCM']]:\n",
    "            continue\n",
    "\n",
    "        start_time = annotation['Starttime']\n",
    "        end_time = annotation['Endtime']\n",
    "\n",
    "        # Calculate the event center time\n",
    "        event_center = (start_time + end_time) / 2\n",
    "\n",
    "        # Calculate the segment start and end times to ensure a 1-second duration\n",
    "        segment_start = max(0, event_center - 0.5)  # Ensure the start time is not negative\n",
    "        segment_end = segment_start + 1.0  # End time is 1 second after the start time\n",
    "\n",
    "        # Extract the segment from the audio file\n",
    "        start_sample = int(segment_start * sr)\n",
    "        end_sample = int(segment_end * sr)\n",
    "        segment = y[start_sample:end_sample]\n",
    "\n",
    "        # If the segment is less than 1 second, pad it with zeros\n",
    "        if len(segment) < sr:\n",
    "            segment = np.pad(segment, (0, sr - len(segment)), 'constant')\n",
    "        \n",
    "        # Upsample the segment to the target sampling rate\n",
    "        upsampled_segment = upsample_audio(segment, initial_sr=sr, target_sr=target_sr)\n",
    "\n",
    "        # Determine the label based on the annotation\n",
    "        if annotation['SNMK'] == 'POS':\n",
    "            label = 1  # SNMK\n",
    "        elif annotation['CCMK'] == 'POS':\n",
    "            label = 2  # CCMK\n",
    "        elif annotation['AGGM'] == 'POS':\n",
    "            label = 3  # AGGM\n",
    "        elif annotation['SOCM'] == 'POS':\n",
    "            label = 4  # SOCM\n",
    "        else:\n",
    "            label = 7  # unknown\n",
    "\n",
    "        # Count the label\n",
    "        label_count[label] = label_count.get(label, 0) + 1\n",
    "\n",
    "        # Create the output filename based on the audio file name, label, and segment index\n",
    "        output_filename = os.path.join(output_dir, f'{os.path.basename(audio_file).replace(\".wav\", \"\")}_segment_{i+1}_label_{label}.wav')\n",
    "\n",
    "        # Save the segment as a WAV file using the soundfile library\n",
    "        sf.write(output_filename, upsampled_segment, target_sr)\n",
    "\n",
    "    # Process background noise segments\n",
    "    no_event_segments = []\n",
    "    for i in range(len(y) // sr):\n",
    "        segment_start = i * sr\n",
    "        segment_end = segment_start + sr\n",
    "        segment = y[segment_start:segment_end]\n",
    "\n",
    "        # Check if the segment overlaps with any event\n",
    "        overlaps = any(start_sample <= segment_start <= end_sample or start_sample <= segment_end <= end_sample\n",
    "                       for start_sample, end_sample in zip(annotations['Starttime'] * sr, annotations['Endtime'] * sr))\n",
    "\n",
    "        if not overlaps:\n",
    "            no_event_segments.append(segment)\n",
    "\n",
    "    # Randomly select the same number of no-event segments as there are event segments\n",
    "    np.random.shuffle(no_event_segments)\n",
    "    selected_no_event_segments = no_event_segments[:len(annotations)]\n",
    "\n",
    "    # Save the no-event segments with label 0 (background noise)\n",
    "    for i, segment in enumerate(selected_no_event_segments):\n",
    "        # Upsample background noise segments\n",
    "        upsampled_segment = upsample_audio(segment, initial_sr=sr, target_sr=target_sr)\n",
    "        output_filename = os.path.join(output_dir, f'segment_{os.path.basename(audio_file).replace(\".wav\", \"\")}_{i+1}_label_0.wav')\n",
    "        sf.write(output_filename, upsampled_segment, target_sr)\n",
    "        label_count[0] = label_count.get(0, 0) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Proportions Output:\n",
    "- The output provides the count of segments for each label in the dataset, with labels corresponding to specific meerkat calls or noise.\n",
    "- The output indicates that the 'Noise' (label 0) and 'CCMK' (label 2) classes are dominating the dataset, with a significantly higher number of segments compared to other classes like 'SNMK' (label 1), 'AGGM' (label 3), and 'SOCM' (label 4). This imbalance can skew the model's performance, as it may become biased towards the more frequent classes. \n",
    "- To address this issue, we can apply techniques such as resampling, either by oversampling the underrepresented classes or by selectively sampling the background noise to balance the number of samples per class. Additionally, applying class weighting during model training can help mitigate the effects of this imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label CCMK: 1046 segments\n",
      "Label SOCM: 40 segments\n",
      "Label SNMK: 95 segments\n",
      "Label AGGM: 53 segments\n",
      "Label Noise: 1294 segments\n"
     ]
    }
   ],
   "source": [
    "label_names = {\n",
    "    1: 'SNMK',  \n",
    "    2: 'CCMK',  \n",
    "    3: 'AGGM',  \n",
    "    4: 'SOCM',  \n",
    "    0: 'Noise',\n",
    "    7: 'Unknown'  \n",
    "}\n",
    "\n",
    "# Display the count of segments per label\n",
    "for label, count in label_count.items():\n",
    "    label_name = label_names.get(label, 'Unknown')\n",
    "    print(f'Label {label_name}: {count} segments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Switch off warnings\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# - Import numpy\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "# - Import the plotting library\n",
    "import sys\n",
    "!{sys.executable} -m pip install --quiet matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = [16, 10]\n",
    "plt.rcParams[\"figure.dpi\"] = 300\n",
    "\n",
    "# - Rich printing\n",
    "try:\n",
    "    from rich import print\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - import AFE\n",
    "from rockpool.devices.xylo.syns61201 import AFESim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Sampling frequency (8000.0) must be at least 6 times the highest BPF centre freq. (i.e. >101640 Hz)\n                The main reason is that the microphone produces THD (third-order distortion) which may fallback into the wrong frequency\n                if the sampling frequency is not large enough.\n                ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 14\u001b[0m\n\u001b[1;32m     10\u001b[0m seed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m                         \u001b[38;5;66;03m# Seed for mistmatch generation\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# - Initialize the AFE simulation, and convert it to a high-level `TimedModule`\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m afe \u001b[38;5;241m=\u001b[39m \u001b[43mAFESim\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mraster_period\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mraster_period\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_spike_per_raster_period\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_spike_per_raster_period\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_noise\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43madd_noise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_offset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43madd_offset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_mismatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43madd_mismatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtimed()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/capstone/lib/python3.8/site-packages/rockpool/nn/modules/module.py:689\u001b[0m, in \u001b[0;36mPostInitMetaMixin.__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    688\u001b[0m     \u001b[38;5;66;03m# - Instantiate the object\u001b[39;00m\n\u001b[0;32m--> 689\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    691\u001b[0m     \u001b[38;5;66;03m# - Check for a `__post_init__` method\u001b[39;00m\n\u001b[1;32m    692\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__post_init__\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/capstone/lib/python3.8/site-packages/rockpool/devices/xylo/syns61201/afe_sim_empirical.py:201\u001b[0m, in \u001b[0;36mAFESim.__init__\u001b[0;34m(self, fs, raster_period, max_spike_per_raster_period, add_noise, add_offset, add_mismatch, seed, num_workers, *args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# - Check the filters w.r.t the sampling frequency\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mFs \u001b[38;5;241m<\u001b[39m (\u001b[38;5;241m6\u001b[39m \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdesign_fcs)):\n\u001b[0;32m--> 201\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    202\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mSampling frequency (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mFs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) must be at least 6 times the highest BPF centre freq. (i.e. >\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;241m6\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39mnp\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdesign_fcs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Hz)\u001b[39m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124m        The main reason is that the microphone produces THD (third-order distortion) which may fallback into the wrong frequency\u001b[39m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;124m        if the sampling frequency is not large enough.\u001b[39m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;124m        \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    206\u001b[0m     )\n\u001b[1;32m    208\u001b[0m \u001b[38;5;66;03m# - nominal/design Bandwidths of the filters\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdesign_bws: P_ndarray \u001b[38;5;241m=\u001b[39m SimulationParameter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdesign_fcs \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mQ)\n",
      "\u001b[0;31mValueError\u001b[0m: Sampling frequency (8000.0) must be at least 6 times the highest BPF centre freq. (i.e. >101640 Hz)\n                The main reason is that the microphone produces THD (third-order distortion) which may fallback into the wrong frequency\n                if the sampling frequency is not large enough.\n                "
     ]
    }
   ],
   "source": [
    "# - AFE parameters\n",
    "\n",
    "fs = 8e3                          # The sampling frequency of the input, in Hz\n",
    "raster_period = 10e-3               # The output rasterisation time-step in seconds\n",
    "max_spike_per_raster_period = 15    # Maximum number of events per output time-step\n",
    "\n",
    "add_noise = False                    # Enables / disables simulated noise generated by the AFE\n",
    "add_offset = False                 # Add mismatch offset to each filter\n",
    "add_mismatch = False                 # Add simualted mismatch to filter parameters\n",
    "seed = None                         # Seed for mistmatch generation\n",
    "\n",
    "# - Initialize the AFE simulation, and convert it to a high-level `TimedModule`\n",
    "\n",
    "afe = AFESim(\n",
    "        fs = fs,\n",
    "        raster_period = raster_period,\n",
    "        max_spike_per_raster_period = max_spike_per_raster_period,\n",
    "        add_noise = add_noise,\n",
    "        add_offset = add_offset,\n",
    "        add_mismatch = add_mismatch,\n",
    "        seed = seed,\n",
    ").timed()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
