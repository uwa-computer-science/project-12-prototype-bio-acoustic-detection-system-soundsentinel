{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Evaluation with Spike Count and Spike Timed Accuracy\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_snn_performance(model, test_loader, target_label=2, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Evaluate SNN model performance by calculating accuracy, spike count error, and timing precision.\n",
    "    \n",
    "    Args:\n",
    "    - model: Trained SNN model.\n",
    "    - test_loader: DataLoader for test data.\n",
    "    - target_label: Label representing the target class for spike count and accuracy.\n",
    "    - threshold: Threshold to convert spikes to binary predictions.\n",
    "    \n",
    "    Returns:\n",
    "    - accuracy: Overall accuracy of the SNN.\n",
    "    - avg_spike_count_error: Average spike count difference from the expected spike count.\n",
    "    - avg_spike_timing_error: Average timing error between expected and predicted spike times.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    spike_count_errors = []\n",
    "    spike_timing_errors = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.transpose(1, 2)  # Adjust input dimensions if necessary\n",
    "            outputs = model(inputs)  # Forward pass\n",
    "            outputs_sum = outputs.sum(dim=1)  # Sum spikes over time\n",
    "\n",
    "            # Convert to binary predictions based on spike counts\n",
    "            predicted_labels = (outputs_sum > threshold).float()\n",
    "            correct_predictions += (predicted_labels == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "            # Calculate spike count error (difference between expected and actual spike counts)\n",
    "            expected_spikes = (labels == target_label).float()\n",
    "            spike_count_error = torch.abs(outputs_sum - expected_spikes).mean().item()\n",
    "            spike_count_errors.append(spike_count_error)\n",
    "\n",
    "            # Calculate spike timing accuracy (if you have ground truth spike timings)\n",
    "            # Assuming you have true spike times per label, calculate timing differences\n",
    "            # For now, we just store placeholder timing accuracy as zero.\n",
    "            spike_timing_errors.append(0)  # Replace this with actual timing calculation\n",
    "\n",
    "    # Calculate overall accuracy\n",
    "    accuracy = correct_predictions / total_samples\n",
    "    avg_spike_count_error = np.mean(spike_count_errors)\n",
    "    avg_spike_timing_error = np.mean(spike_timing_errors)\n",
    "\n",
    "    return accuracy, avg_spike_count_error, avg_spike_timing_error\n",
    "\n",
    "# Example usage\n",
    "accuracy, avg_spike_count_error, avg_spike_timing_error = evaluate_snn_performance(model, test_loader)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Average Spike Count Error: {avg_spike_count_error:.2f}\")\n",
    "print(f\"Average Spike Timing Error: {avg_spike_timing_error:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision , Recall and F1 score (Binary Classification)\n",
    "\n",
    "def precision_recall_f1(test_outputs, test_labels):\n",
    "    \"\"\"\n",
    "    Calculate precision, recall, and F1 score for binary classification without using external libraries.\n",
    "    \n",
    "    Args:\n",
    "    - test_outputs: List of predicted spike outputs (binary).\n",
    "    - test_labels: List of ground truth labels (binary).\n",
    "    \n",
    "    Returns:\n",
    "    - precision: Precision score.\n",
    "    - recall: Recall score.\n",
    "    - f1_score: F1 score.\n",
    "    \"\"\"\n",
    "    tp = np.sum((test_outputs == 1) & (test_labels == 1))\n",
    "    fp = np.sum((test_outputs == 1) & (test_labels == 0))\n",
    "    fn = np.sum((test_outputs == 0) & (test_labels == 1))\n",
    "\n",
    "    precision = tp / (tp + fp + 1e-6)  # Add small value to avoid division by zero\n",
    "    recall = tp / (tp + fn + 1e-6)\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall + 1e-6)\n",
    "    \n",
    "    return precision, recall, f1_score\n",
    "\n",
    "# Assuming test_outputs and test_labels are collected as binary arrays\n",
    "precision, recall, f1_score = precision_recall_f1(test_outputs, test_labels)\n",
    "\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1_score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualisation of Input and Output \n",
    "# Visualization of input spikes\n",
    "plt.figure(figsize=(10, 6))\n",
    "input_spikes = spikes_tensor[:100, :10].numpy()  # Visualizing the first 100 samples and 10 features\n",
    "plt.imshow(input_spikes.T, cmap='binary', interpolation='nearest')\n",
    "plt.colorbar(label='Spike Activity')\n",
    "plt.title('Input Spike Trains (First 100 Samples)')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Neurons/Features')\n",
    "plt.show()\n",
    "\n",
    "# Visualization of output spikes (predicted labels)\n",
    "plt.figure(figsize=(10, 6))\n",
    "predicted_spikes = outputs[:100].detach().numpy()  # First 100 output spike trains\n",
    "plt.imshow(predicted_spikes.T, cmap='binary', interpolation='nearest')\n",
    "plt.colorbar(label='Spike Activity')\n",
    "plt.title('Predicted Output Spike Trains (First 100 Samples)')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Output Neurons')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Measuring Accuracy \n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# Calculate accuracy\n",
    "for i in range(len(test_outputs)):\n",
    "    predicted_label = 1 if test_outputs[i] > 0.5 else 0  # Assuming binary classification and sigmoid activation\n",
    "    if predicted_label == test_labels[i]:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
