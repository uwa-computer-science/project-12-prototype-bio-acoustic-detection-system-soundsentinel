{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is not available. PyTorch will use the CPU instead.\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available(): \n",
    "    print(\"CUDA is available! PyTorch can use the GPU.\")\n",
    "    print(f\"Number of GPUs available: {torch.cuda.device_count()}\")\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"CUDA is not available. PyTorch will use the CPU instead.\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import soundfile as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths for the audio files and corresponding CSV annotations\n",
    "audio_files = ['/Users/xiaoyuliu/Documents/school/capstone/MT/dcase_MK1.wav', '/Users/xiaoyuliu/Documents/school/capstone/MT/dcase_MK2.wav']\n",
    "csv_files = ['/Users/xiaoyuliu/Documents/school/capstone/MT/dcase_MK1.csv', '/Users/xiaoyuliu/Documents/school/capstone/MT/dcase_MK2.csv']\n",
    "\n",
    "# Output directory for AFE segments\n",
    "output_dir = 'processed_segments_new'\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from Esther's preprocessing code for audio segmentation from processed_segments.py\n",
    "# Target sampling rate\n",
    "sr = 8000\n",
    "\n",
    "# Upsampling target rate\n",
    "target_sr = 101640\n",
    "\n",
    "# Initialize a dictionary to keep track of the number of segments for each label\n",
    "label_count = {}\n",
    "\n",
    "# Function to upsample audio segments\n",
    "def upsample_audio(segment, initial_sr, target_sr):\n",
    "    duration = len(segment) / initial_sr\n",
    "    initial_time_points = np.linspace(0, duration, len(segment))\n",
    "    target_time_points = np.linspace(0, duration, int(duration * target_sr))\n",
    "    upsampled_segment = np.interp(target_time_points, initial_time_points, segment) # Perform linear interpolation to resample the original audio segment to the new time points\n",
    "    return upsampled_segment\n",
    "\n",
    "# Iterate through each audio file and its corresponding CSV annotation file\n",
    "for audio_file, csv_file in zip(audio_files, csv_files):\n",
    "    # Load the audio file\n",
    "    y, sr = librosa.load(audio_file, sr=sr)\n",
    "\n",
    "    # Load the CSV annotations\n",
    "    annotations = pd.read_csv(csv_file)\n",
    "\n",
    "    # Iterate over each row in the CSV to extract annotation details\n",
    "    for i, annotation in annotations.iterrows():\n",
    "        # Skip the segment if any of the annotation columns contain 'UNK'\n",
    "        if 'UNK' in [annotation['SNMK'], annotation['CCMK'], annotation['AGGM'], annotation['SOCM']]:\n",
    "            continue\n",
    "\n",
    "        start_time = annotation['Starttime']\n",
    "        end_time = annotation['Endtime']\n",
    "\n",
    "        # Calculate the event center time\n",
    "        event_center = (start_time + end_time) / 2\n",
    "\n",
    "        # Calculate the segment start and end times to ensure a 1-second duration\n",
    "        segment_start = max(0, event_center - 0.5)  # Ensure the start time is not negative\n",
    "        segment_end = segment_start + 1.0  # End time is 1 second after the start time\n",
    "\n",
    "        # Extract the segment from the audio file\n",
    "        start_sample = int(segment_start * sr)\n",
    "        end_sample = int(segment_end * sr)\n",
    "        segment = y[start_sample:end_sample]\n",
    "\n",
    "        # If the segment is less than 1 second, pad it with zeros\n",
    "        if len(segment) < sr:\n",
    "            segment = np.pad(segment, (0, sr - len(segment)), 'constant')\n",
    "        \n",
    "        # Upsample the segment to the target sampling rate\n",
    "        upsampled_segment = upsample_audio(segment, initial_sr=sr, target_sr=target_sr)\n",
    "\n",
    "        # Determine the label based on the annotation\n",
    "        if annotation['SNMK'] == 'POS':\n",
    "            label = 1  # SNMK\n",
    "        elif annotation['CCMK'] == 'POS':\n",
    "            label = 2  # CCMK\n",
    "        elif annotation['AGGM'] == 'POS':\n",
    "            label = 3  # AGGM\n",
    "        elif annotation['SOCM'] == 'POS':\n",
    "            label = 4  # SOCM\n",
    "        else:\n",
    "            label = 7  # unknown\n",
    "\n",
    "        # Count the label\n",
    "        label_count[label] = label_count.get(label, 0) + 1\n",
    "\n",
    "        # Create the output filename based on the audio file name, label, and segment index\n",
    "        output_filename = os.path.join(output_dir, f'{os.path.basename(audio_file).replace(\".wav\", \"\")}_segment_{i+1}_label_{label}.wav')\n",
    "\n",
    "        # Save the segment as a WAV file using the soundfile library\n",
    "        sf.write(output_filename, upsampled_segment, target_sr)\n",
    "\n",
    "    # Process background noise segments\n",
    "    no_event_segments = []\n",
    "    for i in range(len(y) // sr):\n",
    "        segment_start = i * sr\n",
    "        segment_end = segment_start + sr\n",
    "        segment = y[segment_start:segment_end]\n",
    "\n",
    "        # Check if the segment overlaps with any event\n",
    "        overlaps = any(start_sample <= segment_start <= end_sample or start_sample <= segment_end <= end_sample\n",
    "                       for start_sample, end_sample in zip(annotations['Starttime'] * sr, annotations['Endtime'] * sr))\n",
    "\n",
    "        if not overlaps:\n",
    "            no_event_segments.append(segment)\n",
    "\n",
    "    # Randomly select the same number of no-event segments as there are event segments\n",
    "    np.random.shuffle(no_event_segments)\n",
    "    selected_no_event_segments = no_event_segments[:len(annotations)]\n",
    "\n",
    "    # Save the no-event segments with label 0 (background noise)\n",
    "    for i, segment in enumerate(selected_no_event_segments):\n",
    "        # Upsample background noise segments\n",
    "        upsampled_segment = upsample_audio(segment, initial_sr=sr, target_sr=target_sr)\n",
    "        output_filename = os.path.join(output_dir, f'segment_{os.path.basename(audio_file).replace(\".wav\", \"\")}_{i+1}_label_0.wav')\n",
    "        sf.write(output_filename, upsampled_segment, target_sr)\n",
    "        label_count[0] = label_count.get(0, 0) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Proportions Output:\n",
    "- The output provides the count of segments for each label in the dataset, with labels corresponding to specific meerkat calls or noise.\n",
    "- The output indicates that the 'Noise' (label 0) and 'CCMK' (label 2) classes are dominating the dataset, with a significantly higher number of segments compared to other classes like 'SNMK' (label 1), 'AGGM' (label 3), and 'SOCM' (label 4). This imbalance can skew the model's performance, as it may become biased towards the more frequent classes. \n",
    "- To address this issue, we can apply techniques such as resampling, either by oversampling the underrepresented classes or by selectively sampling the background noise to balance the number of samples per class. Additionally, applying class weighting during model training can help mitigate the effects of this imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label CCMK: 1046 segments\n",
      "Label SOCM: 40 segments\n",
      "Label SNMK: 95 segments\n",
      "Label AGGM: 53 segments\n",
      "Label Noise: 1294 segments\n"
     ]
    }
   ],
   "source": [
    "label_names = {\n",
    "    1: 'SNMK',  \n",
    "    2: 'CCMK',  \n",
    "    3: 'AGGM',  \n",
    "    4: 'SOCM',  \n",
    "    0: 'Noise',\n",
    "    7: 'Unknown'  \n",
    "}\n",
    "\n",
    "# Display the count of segments per label\n",
    "for label, count in label_count.items():\n",
    "    label_name = label_names.get(label, 'Unknown')\n",
    "    print(f'Label {label_name}: {count} segments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Switch off warnings\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# - Import numpy\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "# - Import the plotting library\n",
    "import sys\n",
    "!{sys.executable} -m pip install --quiet matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = [16, 10]\n",
    "plt.rcParams[\"figure.dpi\"] = 300\n",
    "\n",
    "# - Rich printing\n",
    "try:\n",
    "    from rich import print\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - import AFE\n",
    "from rockpool.devices.xylo.syns61201 import AFESim\n",
    "# - AFE parameters\n",
    "\n",
    "fs = 1.0164e5                       # The sampling frequency of the input, in Hz\n",
    "raster_period = 10e-3               # The output rasterisation time-step in seconds\n",
    "max_spike_per_raster_period = 15    # Maximum number of events per output time-step\n",
    "\n",
    "add_noise = False                    # Enables / disables simulated noise generated by the AFE\n",
    "add_offset = False                 # Add mismatch offset to each filter\n",
    "add_mismatch = False                 # Add simualted mismatch to filter parameters\n",
    "seed = None                         # Seed for mistmatch generation\n",
    "\n",
    "# - Initialize the AFE simulation, and convert it to a high-level `TimedModule`\n",
    "\n",
    "afe = AFESim(\n",
    "        fs = fs,\n",
    "        raster_period = raster_period,\n",
    "        max_spike_per_raster_period = max_spike_per_raster_period,\n",
    "        add_noise = add_noise,\n",
    "        add_offset = add_offset,\n",
    "        add_mismatch = add_mismatch,\n",
    "        seed = seed,\n",
    ").timed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding max length: 100%|██████████| 2528/2528 [53:27<00:00,  1.27s/file]   \n",
      "Processing audio files: 100%|██████████| 2528/2528 [1:12:16<00:00,  1.72s/file]  \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import librosa\n",
    "from tqdm import tqdm \n",
    "from rockpool.timeseries import TSContinuous\n",
    "\n",
    "# Check if CUDA is available and set the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Directory containing audio segments\n",
    "audio_dir = 'processed_segments_new'\n",
    "\n",
    "# Initialize lists to store audio segments and their corresponding labels\n",
    "all_spikes = []\n",
    "all_labels = []\n",
    "\n",
    "# List of files in the directory\n",
    "file_list = [filename for filename in os.listdir(audio_dir) if filename.endswith(\".wav\")]\n",
    "\n",
    "max_length = 0\n",
    "\n",
    "# Find max length of spike data\n",
    "for filename in tqdm(file_list, desc=\"Finding max length\", unit=\"file\"):\n",
    "    audio_file_path = os.path.join(audio_dir, filename)\n",
    "    segment, sr = librosa.load(audio_file_path, sr=fs)  \n",
    "    dt = 1 / fs\n",
    "    inp_ts = TSContinuous.from_clocked(segment, dt=dt, periodic=True)\n",
    "    filt_spikes, state, rec = afe(inp_ts, record=True)\n",
    "\n",
    "    spike_data = filt_spikes.raster(dt=10e-3, add_events=True).T\n",
    "    max_length = max(max_length, spike_data.shape[1])\n",
    "\n",
    "# Function to save checkpoint\n",
    "def save_checkpoint(file_idx, all_spikes, all_labels):\n",
    "    checkpoint_dir = 'checkpoints'\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    torch.save(all_spikes, f'{checkpoint_dir}/spike_data_tensor_{file_idx}.pt')\n",
    "    torch.save(all_labels, f'{checkpoint_dir}/labels_tensor_{file_idx}.pt')\n",
    "\n",
    "# Process audio files and pad spike data\n",
    "for file_idx, filename in enumerate(tqdm(file_list, desc=\"Processing audio files\", unit=\"file\")):\n",
    "    # Extract label from filename\n",
    "    label = int(filename.split(\"_\")[-1].replace(\".wav\", \"\"))\n",
    "\n",
    "    audio_file_path = os.path.join(audio_dir, filename)\n",
    "    segment, sr = librosa.load(audio_file_path, sr=fs)  \n",
    "    dt = 1 / fs\n",
    "    inp_ts = TSContinuous.from_clocked(segment, dt=dt, periodic=True)\n",
    "    filt_spikes, state, rec = afe(inp_ts, record=True)\n",
    "\n",
    "    # Convert spikes into numpy array\n",
    "    spike_data = filt_spikes.raster(dt=10e-3, add_events=True).T\n",
    "\n",
    "    # Pad spike data to match max length\n",
    "    pad_width = max_length - spike_data.shape[1]\n",
    "    spike_data = np.pad(spike_data, ((0, 0), (0, pad_width)), 'constant')\n",
    "\n",
    "    # Convert to PyTorch tensor and move to GPU\n",
    "    all_spikes.append(torch.tensor(spike_data, dtype=torch.float32).to(device))\n",
    "    all_labels.append(label)\n",
    "\n",
    "    # Save checkpoint every checkpoint_interval files\n",
    "    checkpoint_interval = 100\n",
    "    if (file_idx + 1) % checkpoint_interval == 0:\n",
    "        save_checkpoint(file_idx + 1, all_spikes, all_labels)\n",
    "\n",
    "# Final save of all tensors to disk\n",
    "all_spikes_tensor = torch.stack(all_spikes).to(device)\n",
    "all_labels_tensor = torch.tensor(all_labels).to(device)\n",
    "\n",
    "os.makedirs('tensors', exist_ok=True)\n",
    "torch.save(all_spikes_tensor, 'tensors/spike_data_tensor.pt')\n",
    "torch.save(all_labels_tensor, 'tensors/labels_tensor.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Label <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1294</span> samples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Label \u001b[1;36m0\u001b[0m: \u001b[1;36m1294\u001b[0m samples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Label <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">95</span> samples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Label \u001b[1;36m1\u001b[0m: \u001b[1;36m95\u001b[0m samples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Label <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1046</span> samples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Label \u001b[1;36m2\u001b[0m: \u001b[1;36m1046\u001b[0m samples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Label <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">53</span> samples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Label \u001b[1;36m3\u001b[0m: \u001b[1;36m53\u001b[0m samples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Label <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">40</span> samples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Label \u001b[1;36m4\u001b[0m: \u001b[1;36m40\u001b[0m samples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculate and print the count of each label\n",
    "unique_labels, label_counts = torch.unique(all_labels_tensor, return_counts=True)\n",
    "for label, count in zip(unique_labels.tolist(), label_counts.tolist()):\n",
    "    print(f\"Label {label}: {count} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class CCMKDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, spikes_tensor, labels_tensor, target_label=2):\n",
    "        self.spikes_tensor = spikes_tensor\n",
    "        self.labels_tensor = labels_tensor\n",
    "        self.target_label = target_label\n",
    "\n",
    "        # Separate background noise (Label 0)\n",
    "        background_noise_mask = (self.labels_tensor == 0)\n",
    "        # Create an event mask for the target label and other labels (Label 1, 3, 4)\n",
    "        event_mask = (self.labels_tensor == target_label) | (self.labels_tensor == 1) | \\\n",
    "                     (self.labels_tensor == 3) | (self.labels_tensor == 4)\n",
    "\n",
    "        # Calculate the number of samples to select from Label 0 (background noise)=label2-1-3-4\n",
    "        required_samples_from_label_0 = max(0, event_mask.sum().item() - (self.labels_tensor == 1).sum().item() - \n",
    "                                            (self.labels_tensor == 3).sum().item() - (self.labels_tensor == 4).sum().item())\n",
    "\n",
    "        # Check if required samples exceed the available Label 0 samples\n",
    "        if required_samples_from_label_0 > background_noise_mask.sum().item():\n",
    "            required_samples_from_label_0 = background_noise_mask.sum().item()\n",
    "\n",
    "        # Randomly select a subset of background noise\n",
    "        selected_background_noise = torch.where(background_noise_mask)[0]\n",
    "        selected_background_noise = np.random.choice(selected_background_noise.cpu(), required_samples_from_label_0, replace=False)\n",
    "        \n",
    "        # Combine event_mask and selected background noise indices\n",
    "        self.selected_mask = torch.cat([torch.where(event_mask)[0], torch.tensor(selected_background_noise)])\n",
    "        \n",
    "        # Combine selected background noise with events and convert non-target labels to 'unknown'\n",
    "        self.filtered_spikes = self.spikes_tensor[self.selected_mask]\n",
    "        self.filtered_labels = self.labels_tensor[self.selected_mask]\n",
    "        self.filtered_labels[self.filtered_labels != target_label] = 0  # Label non-targets as 'unknown'\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filtered_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.filtered_spikes[idx], self.filtered_labels[idx]\n",
    "\n",
    "# Example usage:\n",
    "# Assuming all_spikes_tensor and all_labels_tensor are already defined\n",
    "dataset = CCMKDataset(spikes_tensor=all_spikes_tensor, labels_tensor=all_labels_tensor)\n",
    "\n",
    "# Create a DataLoader to batch the data\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
