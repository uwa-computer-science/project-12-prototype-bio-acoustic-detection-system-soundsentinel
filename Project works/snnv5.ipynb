{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "hdAtTAqBIM_7",
        "outputId": "c49eb6d9-a330-4a83-c73c-eb96a8419d1f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nv2:\\ndataset:\\nignoring labels 1, 3, and 4, retaining only labels 0 (background noise) and label 2 (target class).\\nbackground noise (label 0) is downsampled to ensure it matches the number of samples with label 2\\nsplit the dataset using torch.utils.data.random_split, into 80% for the training set, 10% for the test set, and 10% for the validation set\\nmodel:\\nloss function modified to (MSE) loss\\noutput: sums the spikes along time dimension\\nvisualize the internal activity\\n\\nv3:\\nsplit the dataset using random_split\\nData Augmentation: for train_loader label 1\\n  time_shift: Shifts the event data along the time axis.\\n  add_noise: Adds Gaussian noise to event data.\\n  random_drop: Randomly drops parts of the event data.\\nadd Weights and Biases to track training results\\ntry differnt output: \\noutput = output.sum(dim=1) ,targets.float().unsqueeze(1)\\noutput = output.mean(dim=1) \\nand synnet with different hidden layer\\nthe best model is synnet with 32 hidden layer use augmented data,and mean around each time series. the accuracy rate is 80%\\n\\nv4: \\noutput = output.sum(dim=1) +target firing rate tensor\\nadjust data augmentation parameter(smaller)\\n\\n\\nv5:\\nsynnet:  dt=10e-3,tau_syn_base/tau_mem/tau_syn_out=0.02\\ntry output=\"vmem\"\\n \\n'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "v2:\n",
        "dataset:\n",
        "ignoring labels 1, 3, and 4, retaining only labels 0 (background noise) and label 2 (target class).\n",
        "background noise (label 0) is downsampled to ensure it matches the number of samples with label 2\n",
        "split the dataset using torch.utils.data.random_split, into 80% for the training set, 10% for the test set, and 10% for the validation set\n",
        "model:\n",
        "loss function modified to (MSE) loss\n",
        "output: sums the spikes along time dimension\n",
        "visualize the internal activity\n",
        "\n",
        "v3:\n",
        "split the dataset using random_split\n",
        "Data Augmentation: for train_loader label 1\n",
        "  time_shift: Shifts the event data along the time axis.\n",
        "  add_noise: Adds Gaussian noise to event data.\n",
        "  random_drop: Randomly drops parts of the event data.\n",
        "add Weights and Biases to track training results\n",
        "try differnt output: \n",
        "output = output.sum(dim=1) ,targets.float().unsqueeze(1)\n",
        "output = output.mean(dim=1) \n",
        "and synnet with different hidden layer\n",
        "the best model is synnet with 32 hidden layer use augmented data,and mean around each time series. the accuracy rate is 80%\n",
        "\n",
        "v4: \n",
        "output = output.sum(dim=1) +target firing rate tensor\n",
        "adjust data augmentation parameter(smaller)\n",
        "\n",
        "\n",
        "v5:\n",
        "synnet:  dt=10e-3,tau_syn_base/tau_mem/tau_syn_out=0.02\n",
        "try output=\"vmem\"\n",
        " \n",
        "\"\"\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZE_AoYPs1Jr",
        "outputId": "cf024937-d09a-480e-f854-6d5d3d8ef60b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Labels distribution: tensor([1294,   95, 1046,   53,   40])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\dines\\AppData\\Local\\Temp\\ipykernel_33984\\240507397.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  labels_tensor = torch.load(\"C:/Users/dines/Capstone Project/tensors/labels_tensor.pt\")\n",
            "C:\\Users\\dines\\AppData\\Local\\Temp\\ipykernel_33984\\240507397.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  spikes_tensor = torch.load(\"C:/Users/dines/Capstone Project/tensors/spike_data_tensor.pt\")\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Load tensor\n",
        "labels_tensor = torch.load(\"C:/Users/dines/Capstone Project/tensors/labels_tensor.pt\")\n",
        "spikes_tensor = torch.load(\"C:/Users/dines/Capstone Project/tensors/spike_data_tensor.pt\")\n",
        "\n",
        "label_distribution = torch.bincount(labels_tensor)\n",
        "print(f'Original Labels distribution: {label_distribution}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UH-3VSb4ve9A"
      },
      "source": [
        "# dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9nSAyTIsrym",
        "outputId": "70a1de29-9f90-47cc-ff5c-c4dbc426f359"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of downsampled label 0 samples: 1046\n",
            "Number of target label 1 samples: 1046\n",
            "Balanced dataset length: 2092\n",
            "Filtered Labels distribution after processing: tensor([1046, 1046])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "class CCMKDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, spikes_tensor, labels_tensor, target_label=2):\n",
        "        self.spikes_tensor = spikes_tensor\n",
        "        self.labels_tensor = labels_tensor\n",
        "        self.target_label = target_label\n",
        "\n",
        "        # Step 1: Ignore labels 1, 3, and 4\n",
        "        valid_mask = (self.labels_tensor == 0) | (self.labels_tensor == target_label)\n",
        "        self.spikes_tensor = self.spikes_tensor[valid_mask]\n",
        "        self.labels_tensor = self.labels_tensor[valid_mask]\n",
        "\n",
        "        # Convert the target label (2) to 1 (positive sample)\n",
        "        self.labels_tensor[self.labels_tensor == target_label] = 1\n",
        "\n",
        "        # Step 2: Downsample label 0 (background noise) to match the number of target samples (label 2 -> now 1)\n",
        "        label_0_indices = torch.where(self.labels_tensor == 0)[0]\n",
        "        label_1_indices = torch.where(self.labels_tensor == 1)[0]\n",
        "\n",
        "        # print samples\n",
        "        #print(f\"Number of label 0 samples before downsampling: {len(label_0_indices)}\")\n",
        "        #print(f\"Number of label 1 samples (after converting target label 2 to 1): {len(label_1_indices)}\")\n",
        "\n",
        "        num_samples = min(len(label_1_indices), len(label_0_indices))\n",
        "\n",
        "        # Randomly sample from label 0 and label 1 indices\n",
        "        selected_label_0_indices = torch.tensor(np.random.choice(label_0_indices.cpu(), size=num_samples, replace=False))\n",
        "        selected_label_1_indices = torch.tensor(np.random.choice(label_1_indices.cpu(), size=num_samples, replace=False))\n",
        "\n",
        "        # Combine the downsampled label 0 indices with label 1 indices\n",
        "        balanced_indices = torch.cat([selected_label_0_indices, selected_label_1_indices])\n",
        "\n",
        "        # print data after samples\n",
        "        print(f\"Number of downsampled label 0 samples: {len(selected_label_0_indices)}\")\n",
        "        print(f\"Number of target label 1 samples: {len(selected_label_1_indices)}\")\n",
        "        print(f\"Balanced dataset length: {len(balanced_indices)}\")\n",
        "\n",
        "        # Apply the balanced indices to spikes and labels\n",
        "        self.spikes_tensor = self.spikes_tensor[balanced_indices]\n",
        "        self.labels_tensor = self.labels_tensor[balanced_indices]\n",
        "\n",
        "        # Debugging: Check the distribution of labels after processing\n",
        "        print(f\"Filtered Labels distribution after processing: {torch.bincount(self.labels_tensor.int())}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels_tensor)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.spikes_tensor[idx], self.labels_tensor[idx]\n",
        "\n",
        "dataset = CCMKDataset(spikes_tensor=spikes_tensor, labels_tensor=labels_tensor)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eax4dl-9vh0y"
      },
      "source": [
        "# split and dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "k-RVmFtMsrym"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader, Subset, random_split\n",
        "\n",
        "\n",
        "# Balance the dataset by selecting equal samples from both classes\n",
        "label_0_indices = torch.where(dataset.labels_tensor == 0)[0]\n",
        "label_1_indices = torch.where(dataset.labels_tensor == 1)[0]\n",
        "\n",
        "dataset_0 = torch.utils.data.Subset(dataset, label_0_indices)\n",
        "dataset_1 = torch.utils.data.Subset(dataset, label_1_indices)\n",
        "\n",
        "# Split size\n",
        "train_size_0 = int(0.8 * len(dataset_0))\n",
        "val_size_0 = int(0.1 * len(dataset_0))\n",
        "test_size_0 = len(dataset_0) - train_size_0 - val_size_0\n",
        "\n",
        "train_size_1 = int(0.8 * len(dataset_1))\n",
        "val_size_1 = int(0.1 * len(dataset_1))\n",
        "test_size_1 = len(dataset_1) - train_size_1 - val_size_1\n",
        "\n",
        "# Split the dataset\n",
        "train_dataset_0, val_dataset_0, test_dataset_0 = random_split(dataset_0, [train_size_0, val_size_0, test_size_0], generator=torch.Generator().manual_seed(42))\n",
        "train_dataset_1, val_dataset_1, test_dataset_1 = random_split(dataset_1, [train_size_1, val_size_1, test_size_1], generator=torch.Generator().manual_seed(42))\n",
        "\n",
        "# Combine splits from both classes\n",
        "train_dataset = torch.utils.data.ConcatDataset([train_dataset_0, train_dataset_1])\n",
        "val_dataset = torch.utils.data.ConcatDataset([val_dataset_0, val_dataset_1])\n",
        "test_dataset = torch.utils.data.ConcatDataset([test_dataset_0, test_dataset_1])\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KlUuFzqAZCLl",
        "outputId": "daf3ee57-22ac-4e8e-918f-8acaa5c1c176"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train dataset size: 1672\n",
            "Test dataset size: 212\n",
            "Validation dataset size: 208\n",
            "Batch 1:\n",
            " - Inputs shape: torch.Size([32, 16, 101])\n",
            " - Targets shape: torch.Size([32])\n",
            "Train Labels distribution: tensor([836, 836])\n",
            "Validation Labels distribution: tensor([104, 104])\n",
            "Test Labels distribution: tensor([106, 106])\n"
          ]
        }
      ],
      "source": [
        "# Print the shape of the first sample in the dataset\n",
        "print(f\"Train dataset size: {len(train_dataset)}\")\n",
        "print(f\"Test dataset size: {len(test_dataset)}\")\n",
        "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
        "\n",
        "# Check batch information in the train_loader\n",
        "for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "    print(f\"Batch {batch_idx + 1}:\")\n",
        "    print(f\" - Inputs shape: {inputs.shape}\")  # Batch size x Number of channels x Number of time steps\n",
        "    print(f\" - Targets shape: {targets.shape}\")\n",
        "    break  # Only view the first batch\n",
        "\n",
        "# Calculate and print the labels distribution in each loader\n",
        "\n",
        "# Training data label distribution\n",
        "train_labels = torch.cat([batch[1] for batch in train_loader])\n",
        "print(f'Train Labels distribution: {torch.bincount(train_labels.int())}')\n",
        "\n",
        "# Validation data label distribution\n",
        "val_labels = torch.cat([batch[1] for batch in val_loader])\n",
        "print(f'Validation Labels distribution: {torch.bincount(val_labels.int())}')\n",
        "\n",
        "# Test data label distribution\n",
        "test_labels = torch.cat([batch[1] for batch in test_loader])\n",
        "print(f'Test Labels distribution: {torch.bincount(test_labels.int())}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PElTL2thvyw4"
      },
      "source": [
        "# model-vmem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\dines\\miniconda3\\envs\\capstone\\lib\\site-packages\\rockpool\\nn\\networks\\__init__.py:15: UserWarning: This module needs to be ported to teh v2 API.\n",
            "  warnings.warn(f\"{err}\")\n",
            "c:\\Users\\dines\\miniconda3\\envs\\capstone\\lib\\site-packages\\rockpool\\nn\\networks\\__init__.py:20: UserWarning: This module needs to be ported to the v2 API.\n",
            "  warnings.warn(f\"{err}\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(self, n_classes: int, n_channels: int, size_hidden_layers: List = [60], time_constants_per_layer: List = [10], tau_syn_base: float = 0.002, tau_mem: float = 0.002, tau_syn_out: float = 0.002, quantize_time_constants: bool = True, threshold: float = 1.0, threshold_out: Union[float, List[float]] = None, train_threshold: bool = False, neuron_model: Type = <class 'rockpool.nn.modules.torch.lif_torch.LIFTorch'>, max_spikes_per_dt: int = 31, max_spikes_per_dt_out: int = 1, p_dropout: float = 0.0, dt: float = 0.001, output: str = 'spikes', *args, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "import inspect\n",
        "from rockpool.nn.networks import SynNet\n",
        "\n",
        "synnet_signature = inspect.signature(SynNet.__init__)\n",
        "print(synnet_signature)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwpnXLa5Yl0i",
        "outputId": "e6a7cff4-1299-486c-bfc8-2b976a4b5858"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SynNet  with shape (16, 1) {\n",
            "    TorchSequential 'seq' with shape (16, 1) {\n",
            "        LinearTorch '0_LinearTorch' with shape (16, 24)\n",
            "        LIFTorch '1_LIFTorch' with shape (24, 24)\n",
            "        TimeStepDropout '2_TimeStepDropout' with shape (24,)\n",
            "        LinearTorch '3_LinearTorch' with shape (24, 24)\n",
            "        LIFTorch '4_LIFTorch' with shape (24, 24)\n",
            "        TimeStepDropout '5_TimeStepDropout' with shape (24,)\n",
            "        LinearTorch '6_LinearTorch' with shape (24, 24)\n",
            "        LIFTorch '7_LIFTorch' with shape (24, 24)\n",
            "        TimeStepDropout '8_TimeStepDropout' with shape (24,)\n",
            "        LinearTorch '9_LinearTorch' with shape (24, 1)\n",
            "        LIFTorch '10_LIFTorch' with shape (1, 1)\n",
            "    }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.optim import Adam\n",
        "from torch.nn import MSELoss\n",
        "from rockpool.nn.networks import SynNet\n",
        "\n",
        "# Define dataset characteristics\n",
        "n_channels = 16  # Number of input channels\n",
        "n_classes = 1    # the output of MSELoss is discrite\n",
        "dt=10e-3          #rasterization time-step\n",
        "n_time = 100    # Number of time steps 1/10e-3\n",
        "batch_size = 32  # Batch size\n",
        "\n",
        "# Initialize the SynNet model\n",
        "net = SynNet(\n",
        "    output=\"vmem\",                        # Use the membrane potential as the output of the network\n",
        "    p_dropout = 0.1,                      # Dropout proportion to use\n",
        "    n_channels=16,                # Number of input channels\n",
        "    n_classes=1,                  # Number of output classes \n",
        "    size_hidden_layers=[24, 24, 24],      # Number of neurons in each hidden layer\n",
        "    time_constants_per_layer=[2, 4, 8],   # Time constants for each layer\n",
        "    dt=10e-3,\n",
        "    tau_syn_base=0.02,         # at least 2 times more than dt, how long it takes for the synaptic current to decay to a certain fraction  of its peak value.\n",
        "    tau_mem=0.02,\n",
        "    tau_syn_out=0.02)\n",
        "print(net)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "id": "1GxXaAausryn",
        "outputId": "1604ca80-2546-4b50-caa5-6adb324bb58e"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot synaptic time constants for each LIF layer\n",
        "for lyr_name in net.lif_names:\n",
        "    tau_syn = net.seq[lyr_name].tau_syn.detach().cpu().numpy() / 1e-3  # Convert to ms\n",
        "    plt.plot(tau_syn, label=f\"{lyr_name}\")\n",
        "    \n",
        "plt.xlabel(\"Neuron ID\")\n",
        "plt.ylabel(\"Synaptic time constant (ms)\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)              # Set seed for Python random module\n",
        "    np.random.seed(seed)           # Set seed for NumPy\n",
        "    torch.manual_seed(seed)        # Set seed for PyTorch\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)        # Set seed for CUDA\n",
        "        torch.cuda.manual_seed_all(seed)    # Set seed for all GPUs\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.optim import Adam\n",
        "from torch.nn import MSELoss\n",
        "from rockpool.nn.networks import SynNet\n",
        "from tqdm import trange\n",
        "import random \n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m18750937507\u001b[0m (\u001b[33m18750937507-uwa\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\dines\\_netrc\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import wandb\n",
        "# Initialize a new W&B run\n",
        "wandb.login(key=\"2ac9bb7cd9b60300ecd28184faa02b29add8e82b\")\n",
        "#wandb.init(project=\"SNN_meerkat\", name=\"run_with_new_lr\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "vmem_label_0 min: -0.29860177636146545, max: 3.3914458751678467, mean: 0.06312297284603119\n",
            "vmem_label_1 min: -0.3849411606788635, max: 4.67923641204834, mean: 0.09487350285053253\n"
          ]
        }
      ],
      "source": [
        "# Initialize lists to store vmem for different labels\n",
        "vmem_label_0 = []\n",
        "vmem_label_1 = []\n",
        "optimizer = Adam(net.parameters().astorch(), lr=1e-4)\n",
        "# Training loop\n",
        "for inputs, targets in train_loader:\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    # Adjust input dimensions\n",
        "    inputs = inputs.transpose(1, 2)\n",
        "\n",
        "    # Forward pass\n",
        "    outputs, _, _ = net(inputs, record=True)\n",
        "\n",
        "    # Get the vmem from the last LIF layer\n",
        "    vmem = net._record_dict[net.label_last_LIF][\"vmem\"]\n",
        "\n",
        "    # Separate vmem values by labels\n",
        "    vmem_0 = vmem[targets == 0].detach().cpu().numpy()\n",
        "    vmem_1 = vmem[targets == 1].detach().cpu().numpy()\n",
        "\n",
        "    # Store the vmem values for each label\n",
        "    vmem_label_0.append(vmem_0)\n",
        "    vmem_label_1.append(vmem_1)\n",
        "\n",
        "# Concatenate to form a single array for each label\n",
        "vmem_label_0 = np.concatenate(vmem_label_0, axis=0)\n",
        "vmem_label_1 = np.concatenate(vmem_label_1, axis=0)\n",
        "\n",
        "# Check the minimum, maximum, and average values for each label\n",
        "print(f\"vmem_label_0 min: {vmem_label_0.min()}, max: {vmem_label_0.max()}, mean: {vmem_label_0.mean()}\")\n",
        "print(f\"vmem_label_1 min: {vmem_label_1.min()}, max: {vmem_label_1.max()}, mean: {vmem_label_1.mean()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "Finishing last run (ID:imbhi57d) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f177a8cabce8465b9060ca332f256579",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.003 MB of 0.003 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">vmem</strong> at: <a href='https://wandb.ai/18750937507-uwa/snn_v5/runs/imbhi57d' target=\"_blank\">https://wandb.ai/18750937507-uwa/snn_v5/runs/imbhi57d</a><br/> View project at: <a href='https://wandb.ai/18750937507-uwa/snn_v5' target=\"_blank\">https://wandb.ai/18750937507-uwa/snn_v5</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20240926_215407-imbhi57d\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Successfully finished last run (ID:imbhi57d). Initializing new run:<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.18.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\dines\\Capstone Project\\wandb\\run-20240926_215453-ee9uwprg</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/18750937507-uwa/snn_v5/runs/ee9uwprg' target=\"_blank\">vmem</a></strong> to <a href='https://wandb.ai/18750937507-uwa/snn_v5' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/18750937507-uwa/snn_v5' target=\"_blank\">https://wandb.ai/18750937507-uwa/snn_v5</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/18750937507-uwa/snn_v5/runs/ee9uwprg' target=\"_blank\">https://wandb.ai/18750937507-uwa/snn_v5/runs/ee9uwprg</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10, Training Loss: 0.758547121904931, Validation Loss: 1.9540163917200906, Validation Accuracy: 66.35%\n",
            "Epoch 2/10, Training Loss: 0.7756495579796018, Validation Loss: 2.5393102935382297, Validation Accuracy: 63.94%\n",
            "Epoch 3/10, Training Loss: 0.7805092073274109, Validation Loss: 3.4915447746004378, Validation Accuracy: 62.02%\n",
            "Epoch 4/10, Training Loss: 0.509133942284674, Validation Loss: 2.6951927287237987, Validation Accuracy: 62.02%\n",
            "Epoch 5/10, Training Loss: 0.8020151046649465, Validation Loss: 2.6159481150763377, Validation Accuracy: 62.98%\n",
            "Epoch 6/10, Training Loss: 0.5921049683161501, Validation Loss: 2.962100548403604, Validation Accuracy: 62.02%\n",
            "Epoch 7/10, Training Loss: 0.5942276291689783, Validation Loss: 2.8159684198243276, Validation Accuracy: 62.98%\n",
            "Epoch 8/10, Training Loss: 0.5467017512276487, Validation Loss: 3.9095545070511952, Validation Accuracy: 63.94%\n",
            "Epoch 9/10, Training Loss: 0.7219273968125289, Validation Loss: 4.1907609190259665, Validation Accuracy: 56.73%\n",
            "Epoch 10/10, Training Loss: 0.5425209824769002, Validation Loss: 3.638455791132791, Validation Accuracy: 59.13%\n",
            "Test Accuracy: 55.66%\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9be69fd3b3af4784881157fa7f809671",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td></td></tr><tr><td>test_accuracy</td><td></td></tr><tr><td>train_loss</td><td></td></tr><tr><td>val_accuracy</td><td></td></tr><tr><td>val_loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>test_accuracy</td><td>0.5566</td></tr><tr><td>train_loss</td><td>0.54252</td></tr><tr><td>val_accuracy</td><td>0.59135</td></tr><tr><td>val_loss</td><td>3.63846</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">vmem</strong> at: <a href='https://wandb.ai/18750937507-uwa/snn_v5/runs/ee9uwprg' target=\"_blank\">https://wandb.ai/18750937507-uwa/snn_v5/runs/ee9uwprg</a><br/> View project at: <a href='https://wandb.ai/18750937507-uwa/snn_v5' target=\"_blank\">https://wandb.ai/18750937507-uwa/snn_v5</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20240926_215453-ee9uwprg\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# outputs.sum(dim=1),target firing rate tensor\n",
        "\n",
        "\n",
        "wandb.init(project=\"SNN_v5\", config={\n",
        "    \"learning_rate\": 1e-4,\n",
        "    \"epochs\": 10,\n",
        "    \"batch_size\": 32,\n",
        "}, name=\"vmem\")\n",
        "\n",
        "# Access hyperparameters\n",
        "learning_rate = wandb.config.learning_rate\n",
        "n_epochs = wandb.config.epochs\n",
        "batch_size = wandb.config.batch_size\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "optimizer = Adam(net.parameters().astorch(), lr=learning_rate)\n",
        "loss_fun = MSELoss()\n",
        "n_epochs = 10 #change to 100 later\n",
        "seed = 42\n",
        "set_seed(seed)\n",
        "\n",
        "target_vmem_value = 0.6  \n",
        "background_vmem_value = -0.3 \n",
        "\n",
        "\n",
        "# Function to calculate accuracy \n",
        "def calculate_accuracy_vmem(model, data_loader, target_vmem_value):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient calculations\n",
        "        for inputs, targets in data_loader:\n",
        "            inputs = inputs.transpose(1, 2)  # Ensure input shape matches model requirements\n",
        "\n",
        "            outputs, _, _ = net(inputs, record=True)\n",
        "        \n",
        "            vmem = net._record_dict[net.label_last_LIF][\"vmem\"]\n",
        "            outputs = vmem.sum(dim=1)\n",
        "\n",
        "            # Create target vmem\n",
        "            target_vmem = torch.zeros_like(outputs).to(outputs.device)\n",
        "            target_vmem[targets == 1] = target_vmem_value  \n",
        "            target_vmem[targets == 0] = background_vmem_value  \n",
        "\n",
        "            # Use sigmoid activation and classify as 1 if output is above 0.5\n",
        "            predictions = (torch.sigmoid(outputs) > 0.5).float()  \n",
        "\n",
        "\n",
        "            # Compare predictions with target firing rate (binary form)\n",
        "            binary_target = (target_vmem == target_vmem_value).float()\n",
        "\n",
        "            correct += (predictions == binary_target).sum().item()\n",
        "            total += targets.size(0)\n",
        "\n",
        "    accuracy = correct / total\n",
        "    return accuracy\n",
        "\n",
        "# Training and Validation loop\n",
        "for epoch in range(n_epochs):\n",
        "    net.train()  # Set the model to training mode\n",
        "    epoch_loss = 0.0  # Initialize running loss for training\n",
        "\n",
        "    # Training phase\n",
        "    for inputs, targets in train_loader:\n",
        "        optimizer.zero_grad()  # Clear gradients\n",
        "\n",
        "        # Adjust input dimensions to match model expectations\n",
        "        inputs = inputs.transpose(1, 2)\n",
        "\n",
        "        # Forward pass through the network\n",
        "        outputs, _, _ = net(inputs, record=True)  # Output shape [batch_size, n_time, n_classes]\n",
        "        #print(net._record_dict)  \n",
        "              \n",
        "        vmem = net._record_dict[net.label_last_LIF][\"vmem\"]\n",
        "        outputs_sum = vmem.sum(dim=1)  # Sum spikes over the time dimension\n",
        "\n",
        "        # Create target vmem\n",
        "        target_vmem = torch.zeros_like(outputs_sum).to(outputs.device)\n",
        "        target_vmem[targets == 1] = target_vmem_value  \n",
        "        target_vmem[targets == 0] = background_vmem_value  \n",
        "\n",
        "        # Compute the loss using target firing rate\n",
        "        loss = loss_fun(outputs_sum, target_vmem)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumulate loss for this batch\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    # Calculate average training loss for the epoch\n",
        "    avg_train_loss = epoch_loss / len(train_loader)\n",
        "\n",
        "    # Validation phase\n",
        "    net.eval()  # Set the model to evaluation mode\n",
        "    val_loss = 0.0  # Initialize running loss for validation\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in val_loader:\n",
        "            inputs = inputs.transpose(1, 2)\n",
        "            outputs, _, _ = net(inputs, record=True)\n",
        "            vmem = net._record_dict[net.label_last_LIF][\"vmem\"]\n",
        "            outputs_sum = vmem.sum(dim=1)\n",
        "\n",
        "            # Create target vmem\n",
        "            target_vmem = torch.zeros_like(outputs_sum).to(outputs.device)\n",
        "            target_vmem[targets == 1] = target_vmem_value  \n",
        "            target_vmem[targets == 0] = background_vmem_value  \n",
        "\n",
        "            # Compute validation loss\n",
        "            loss = loss_fun(outputs_sum, target_vmem)\n",
        "\n",
        "            # Accumulate validation loss\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    # Calculate average validation loss for the epoch\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "    # Calculate validation accuracy\n",
        "    val_accuracy = calculate_accuracy_vmem(net, val_loader,target_vmem_value)\n",
        "\n",
        "    # Log training loss, validation loss, and accuracy to W&B\n",
        "    wandb.log({\n",
        "        \"epoch\": epoch + 1,\n",
        "        \"train_loss\": avg_train_loss,\n",
        "        \"val_loss\": avg_val_loss,\n",
        "        \"val_accuracy\": val_accuracy\n",
        "    })\n",
        "\n",
        "    # Print training and validation losses, and validation accuracy for the current epoch\n",
        "    print(f'Epoch {epoch + 1}/{n_epochs}, Training Loss: {avg_train_loss}, Validation Loss: {avg_val_loss}, Validation Accuracy: {val_accuracy * 100:.2f}%')\n",
        "\n",
        "# Final evaluation on test data\n",
        "test_accuracy = calculate_accuracy_vmem(net, test_loader, target_vmem_value)\n",
        "\n",
        "# Log test accuracy to W&B\n",
        "wandb.log({\"test_accuracy\": test_accuracy})\n",
        "\n",
        "# Print the final test accuracy\n",
        "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n",
        "\n",
        "artifact = wandb.Artifact('model_vmem', type='model')\n",
        "artifact.add_file(\"model_vmem.pth\")\n",
        "wandb.log_artifact(artifact)\n",
        "\n",
        "# Finish the W&B run\n",
        "wandb.finish()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c:\\Users\\dines\\Capstone Project\\wandb\\run-20240926_210736-87r3ocoh\\files\n"
          ]
        }
      ],
      "source": [
        "print(wandb.run.dir)  # Check the W&B run directory\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'layer_vmems' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[22], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Convert each layer's vmems to a numpy array for processing\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer_name \u001b[38;5;129;01min\u001b[39;00m \u001b[43mlayer_vmems\u001b[49m:\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# Concatenate along the batch dimension to get a single numpy array\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     layer_vmems[layer_name] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(layer_vmems[layer_name], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Shape: (n_batches*batch_size, n_time, n_neurons_per_layer)\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m final vmem shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_vmems[layer_name]\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'layer_vmems' is not defined"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Convert each layer's vmems to a numpy array for processing\n",
        "for layer_name in layer_vmems:\n",
        "    # Concatenate along the batch dimension to get a single numpy array\n",
        "    layer_vmems[layer_name] = np.concatenate(layer_vmems[layer_name], axis=0)  # Shape: (n_batches*batch_size, n_time, n_neurons_per_layer)\n",
        "    print(f\"{layer_name} final vmem shape: {layer_vmems[layer_name].shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# model-spikes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.optim import Adam\n",
        "from torch.nn import MSELoss\n",
        "from rockpool.nn.networks import SynNet\n",
        "\n",
        "# Define dataset characteristics\n",
        "n_channels = 16  # Number of input channels\n",
        "n_classes = 1    # the output of MSELoss is discrite\n",
        "dt=10e-3          #rasterization time-step\n",
        "n_time = 100    # Number of time steps 1/10e-3\n",
        "batch_size = 32  # Batch size\n",
        "\n",
        "# Initialize the SynNet model\n",
        "net.s = SynNet(\n",
        "    p_dropout = 0.1,                      # Dropout proportion to use\n",
        "    n_channels=16,                # Number of input channels\n",
        "    n_classes=1,                  # Number of output classes \n",
        "    size_hidden_layers=[24, 24, 24],      # Number of neurons in each hidden layer\n",
        "    time_constants_per_layer=[2, 4, 8],   # Time constants for each layer\n",
        "    dt=10e-3,\n",
        "    tau_syn_base=0.02,         # at least 2 times more than dt, how long it takes for the synaptic current to decay to a certain fraction  of its peak value.\n",
        "    tau_mem=0.02,\n",
        "    tau_syn_out=0.02)\n",
        "print(net.s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.18.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/xiaoyuliu/Documents/school/capstone/project-12-prototype-bio-acoustic-detection-system-soundsentinel/wandb/run-20240925_153030-i1c9nkxb</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/18750937507-uwa/snn_v5/runs/i1c9nkxb' target=\"_blank\">1e-4</a></strong> to <a href='https://wandb.ai/18750937507-uwa/snn_v5' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/18750937507-uwa/snn_v5' target=\"_blank\">https://wandb.ai/18750937507-uwa/snn_v5</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/18750937507-uwa/snn_v5/runs/i1c9nkxb' target=\"_blank\">https://wandb.ai/18750937507-uwa/snn_v5/runs/i1c9nkxb</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100, Training Loss: 58.04127358490566, Validation Loss: 60.433035714285715, Validation Accuracy: 58.65%\n",
            "Epoch 2/100, Training Loss: 53.01533018867924, Validation Loss: 60.0, Validation Accuracy: 59.62%\n",
            "Epoch 3/100, Training Loss: 47.54599056603774, Validation Loss: 46.888392857142854, Validation Accuracy: 63.94%\n",
            "Epoch 4/100, Training Loss: 48.12264150943396, Validation Loss: 54.848214285714285, Validation Accuracy: 65.87%\n",
            "Epoch 5/100, Training Loss: 43.50235849056604, Validation Loss: 52.0, Validation Accuracy: 68.75%\n",
            "Epoch 6/100, Training Loss: 41.935731132075475, Validation Loss: 40.004464285714285, Validation Accuracy: 68.75%\n",
            "Epoch 7/100, Training Loss: 39.15507075471698, Validation Loss: 44.52232142857143, Validation Accuracy: 71.15%\n",
            "Epoch 8/100, Training Loss: 37.56780660377358, Validation Loss: 41.5625, Validation Accuracy: 71.63%\n",
            "Epoch 9/100, Training Loss: 36.518278301886795, Validation Loss: 53.64732142857143, Validation Accuracy: 74.04%\n",
            "Epoch 10/100, Training Loss: 34.303066037735846, Validation Loss: 55.419642857142854, Validation Accuracy: 73.08%\n",
            "Epoch 11/100, Training Loss: 34.19634433962264, Validation Loss: 50.02232142857143, Validation Accuracy: 74.52%\n",
            "Epoch 12/100, Training Loss: 34.3060141509434, Validation Loss: 40.392857142857146, Validation Accuracy: 75.00%\n",
            "Epoch 13/100, Training Loss: 32.718160377358494, Validation Loss: 43.75, Validation Accuracy: 74.04%\n",
            "Epoch 14/100, Training Loss: 31.35495283018868, Validation Loss: 60.99107142857143, Validation Accuracy: 74.04%\n",
            "Epoch 15/100, Training Loss: 32.0377358490566, Validation Loss: 61.441964285714285, Validation Accuracy: 74.52%\n",
            "Epoch 16/100, Training Loss: 30.764740566037737, Validation Loss: 61.86607142857143, Validation Accuracy: 73.56%\n",
            "Epoch 17/100, Training Loss: 31.5748820754717, Validation Loss: 49.19642857142857, Validation Accuracy: 74.52%\n",
            "Epoch 18/100, Training Loss: 30.275353773584907, Validation Loss: 48.107142857142854, Validation Accuracy: 74.52%\n",
            "Epoch 19/100, Training Loss: 29.722287735849058, Validation Loss: 46.53125, Validation Accuracy: 75.00%\n",
            "Epoch 20/100, Training Loss: 30.11320754716981, Validation Loss: 44.910714285714285, Validation Accuracy: 75.00%\n",
            "Epoch 21/100, Training Loss: 29.652122641509433, Validation Loss: 39.120535714285715, Validation Accuracy: 76.44%\n",
            "Epoch 22/100, Training Loss: 28.837853773584907, Validation Loss: 47.183035714285715, Validation Accuracy: 74.04%\n",
            "Epoch 23/100, Training Loss: 29.932783018867923, Validation Loss: 38.200892857142854, Validation Accuracy: 75.48%\n",
            "Epoch 24/100, Training Loss: 28.130306603773583, Validation Loss: 41.026785714285715, Validation Accuracy: 75.00%\n",
            "Epoch 25/100, Training Loss: 27.806603773584907, Validation Loss: 37.705357142857146, Validation Accuracy: 77.40%\n",
            "Epoch 26/100, Training Loss: 28.275353773584907, Validation Loss: 43.95982142857143, Validation Accuracy: 75.00%\n",
            "Epoch 27/100, Training Loss: 27.839622641509433, Validation Loss: 38.30357142857143, Validation Accuracy: 75.00%\n",
            "Epoch 28/100, Training Loss: 26.754127358490567, Validation Loss: 47.86607142857143, Validation Accuracy: 74.04%\n",
            "Epoch 29/100, Training Loss: 28.38384433962264, Validation Loss: 35.13392857142857, Validation Accuracy: 77.40%\n",
            "Epoch 30/100, Training Loss: 26.952830188679247, Validation Loss: 58.10267857142857, Validation Accuracy: 75.00%\n",
            "Epoch 31/100, Training Loss: 26.816037735849058, Validation Loss: 36.183035714285715, Validation Accuracy: 75.96%\n",
            "Epoch 32/100, Training Loss: 26.611438679245282, Validation Loss: 35.276785714285715, Validation Accuracy: 76.44%\n",
            "Epoch 33/100, Training Loss: 26.548938679245282, Validation Loss: 38.34375, Validation Accuracy: 76.44%\n",
            "Epoch 34/100, Training Loss: 26.44634433962264, Validation Loss: 51.60267857142857, Validation Accuracy: 76.44%\n",
            "Epoch 35/100, Training Loss: 26.28007075471698, Validation Loss: 36.70982142857143, Validation Accuracy: 77.40%\n",
            "Epoch 36/100, Training Loss: 25.60318396226415, Validation Loss: 31.816964285714285, Validation Accuracy: 77.88%\n",
            "Epoch 37/100, Training Loss: 26.72110849056604, Validation Loss: 30.366071428571427, Validation Accuracy: 77.88%\n",
            "Epoch 38/100, Training Loss: 25.819575471698112, Validation Loss: 46.732142857142854, Validation Accuracy: 77.40%\n",
            "Epoch 39/100, Training Loss: 25.82075471698113, Validation Loss: 46.607142857142854, Validation Accuracy: 76.92%\n",
            "Epoch 40/100, Training Loss: 24.87558962264151, Validation Loss: 36.674107142857146, Validation Accuracy: 78.37%\n",
            "Epoch 41/100, Training Loss: 25.504716981132077, Validation Loss: 32.513392857142854, Validation Accuracy: 79.33%\n",
            "Epoch 42/100, Training Loss: 23.973466981132077, Validation Loss: 33.64732142857143, Validation Accuracy: 77.40%\n",
            "Epoch 43/100, Training Loss: 25.25235849056604, Validation Loss: 39.950892857142854, Validation Accuracy: 77.40%\n",
            "Epoch 44/100, Training Loss: 24.52299528301887, Validation Loss: 42.183035714285715, Validation Accuracy: 77.88%\n",
            "Epoch 45/100, Training Loss: 25.0813679245283, Validation Loss: 34.85267857142857, Validation Accuracy: 77.40%\n",
            "Epoch 46/100, Training Loss: 24.659198113207548, Validation Loss: 38.986607142857146, Validation Accuracy: 78.85%\n",
            "Epoch 47/100, Training Loss: 24.360849056603772, Validation Loss: 26.924107142857142, Validation Accuracy: 78.37%\n",
            "Epoch 48/100, Training Loss: 23.46639150943396, Validation Loss: 29.205357142857142, Validation Accuracy: 77.88%\n",
            "Epoch 49/100, Training Loss: 24.73172169811321, Validation Loss: 26.875, Validation Accuracy: 78.85%\n",
            "Epoch 50/100, Training Loss: 23.71108490566038, Validation Loss: 31.183035714285715, Validation Accuracy: 78.85%\n",
            "Epoch 51/100, Training Loss: 24.17747641509434, Validation Loss: 50.013392857142854, Validation Accuracy: 79.33%\n",
            "Epoch 52/100, Training Loss: 23.19870283018868, Validation Loss: 33.816964285714285, Validation Accuracy: 79.33%\n",
            "Epoch 53/100, Training Loss: 23.4498820754717, Validation Loss: 34.89732142857143, Validation Accuracy: 78.37%\n",
            "Epoch 54/100, Training Loss: 24.192216981132077, Validation Loss: 43.017857142857146, Validation Accuracy: 78.85%\n",
            "Epoch 55/100, Training Loss: 23.46992924528302, Validation Loss: 29.941964285714285, Validation Accuracy: 79.33%\n",
            "Epoch 56/100, Training Loss: 23.08372641509434, Validation Loss: 35.857142857142854, Validation Accuracy: 78.85%\n",
            "Epoch 57/100, Training Loss: 24.36320754716981, Validation Loss: 37.495535714285715, Validation Accuracy: 78.37%\n",
            "Epoch 58/100, Training Loss: 23.775943396226417, Validation Loss: 41.13392857142857, Validation Accuracy: 78.85%\n",
            "Epoch 59/100, Training Loss: 23.6875, Validation Loss: 35.017857142857146, Validation Accuracy: 78.85%\n",
            "Epoch 60/100, Training Loss: 23.56073113207547, Validation Loss: 30.897321428571427, Validation Accuracy: 78.85%\n",
            "Epoch 61/100, Training Loss: 22.672759433962263, Validation Loss: 36.96875, Validation Accuracy: 78.85%\n",
            "Epoch 62/100, Training Loss: 23.047759433962263, Validation Loss: 32.183035714285715, Validation Accuracy: 78.37%\n",
            "Epoch 63/100, Training Loss: 24.02004716981132, Validation Loss: 33.3125, Validation Accuracy: 78.37%\n",
            "Epoch 64/100, Training Loss: 23.07075471698113, Validation Loss: 35.42857142857143, Validation Accuracy: 77.40%\n",
            "Epoch 65/100, Training Loss: 22.75766509433962, Validation Loss: 38.424107142857146, Validation Accuracy: 78.37%\n",
            "Epoch 66/100, Training Loss: 22.33313679245283, Validation Loss: 37.013392857142854, Validation Accuracy: 78.85%\n",
            "Epoch 67/100, Training Loss: 22.837264150943398, Validation Loss: 40.07142857142857, Validation Accuracy: 77.88%\n",
            "Epoch 68/100, Training Loss: 23.317806603773583, Validation Loss: 37.058035714285715, Validation Accuracy: 78.85%\n",
            "Epoch 69/100, Training Loss: 21.942216981132077, Validation Loss: 27.816964285714285, Validation Accuracy: 79.81%\n",
            "Epoch 70/100, Training Loss: 21.92570754716981, Validation Loss: 32.95982142857143, Validation Accuracy: 76.44%\n",
            "Epoch 71/100, Training Loss: 24.22995283018868, Validation Loss: 31.196428571428573, Validation Accuracy: 76.92%\n",
            "Epoch 72/100, Training Loss: 23.50884433962264, Validation Loss: 40.169642857142854, Validation Accuracy: 76.92%\n",
            "Epoch 73/100, Training Loss: 22.045990566037737, Validation Loss: 29.522321428571427, Validation Accuracy: 78.37%\n",
            "Epoch 74/100, Training Loss: 21.79186320754717, Validation Loss: 36.879464285714285, Validation Accuracy: 77.40%\n",
            "Epoch 75/100, Training Loss: 22.005306603773583, Validation Loss: 36.705357142857146, Validation Accuracy: 76.44%\n",
            "Epoch 76/100, Training Loss: 22.26120283018868, Validation Loss: 27.53125, Validation Accuracy: 78.85%\n",
            "Epoch 77/100, Training Loss: 22.97995283018868, Validation Loss: 23.049107142857142, Validation Accuracy: 79.81%\n",
            "Epoch 78/100, Training Loss: 21.60141509433962, Validation Loss: 27.25, Validation Accuracy: 79.81%\n",
            "Epoch 79/100, Training Loss: 21.62382075471698, Validation Loss: 31.151785714285715, Validation Accuracy: 77.40%\n",
            "Epoch 80/100, Training Loss: 20.857900943396228, Validation Loss: 34.482142857142854, Validation Accuracy: 77.88%\n",
            "Epoch 81/100, Training Loss: 21.265330188679247, Validation Loss: 33.71875, Validation Accuracy: 77.88%\n",
            "Epoch 82/100, Training Loss: 22.826650943396228, Validation Loss: 25.205357142857142, Validation Accuracy: 79.33%\n",
            "Epoch 83/100, Training Loss: 21.65625, Validation Loss: 32.191964285714285, Validation Accuracy: 78.37%\n",
            "Epoch 84/100, Training Loss: 21.66450471698113, Validation Loss: 30.21875, Validation Accuracy: 77.40%\n",
            "Epoch 85/100, Training Loss: 22.12617924528302, Validation Loss: 32.566964285714285, Validation Accuracy: 77.88%\n",
            "Epoch 86/100, Training Loss: 20.462264150943398, Validation Loss: 27.772321428571427, Validation Accuracy: 77.88%\n",
            "Epoch 87/100, Training Loss: 21.649764150943398, Validation Loss: 32.232142857142854, Validation Accuracy: 77.88%\n",
            "Epoch 88/100, Training Loss: 21.47818396226415, Validation Loss: 27.482142857142858, Validation Accuracy: 78.85%\n",
            "Epoch 89/100, Training Loss: 20.840212264150942, Validation Loss: 35.25892857142857, Validation Accuracy: 76.44%\n",
            "Epoch 90/100, Training Loss: 23.15507075471698, Validation Loss: 22.424107142857142, Validation Accuracy: 79.33%\n",
            "Epoch 91/100, Training Loss: 21.40742924528302, Validation Loss: 28.892857142857142, Validation Accuracy: 78.37%\n",
            "Epoch 92/100, Training Loss: 21.26945754716981, Validation Loss: 32.941964285714285, Validation Accuracy: 77.40%\n",
            "Epoch 93/100, Training Loss: 20.225825471698112, Validation Loss: 31.227678571428573, Validation Accuracy: 76.92%\n",
            "Epoch 94/100, Training Loss: 22.473466981132077, Validation Loss: 26.339285714285715, Validation Accuracy: 78.85%\n",
            "Epoch 95/100, Training Loss: 20.618514150943398, Validation Loss: 37.13392857142857, Validation Accuracy: 74.52%\n",
            "Epoch 96/100, Training Loss: 21.121462264150942, Validation Loss: 31.058035714285715, Validation Accuracy: 77.40%\n",
            "Epoch 97/100, Training Loss: 20.89563679245283, Validation Loss: 24.933035714285715, Validation Accuracy: 77.88%\n",
            "Epoch 98/100, Training Loss: 20.78007075471698, Validation Loss: 42.433035714285715, Validation Accuracy: 74.04%\n",
            "Epoch 99/100, Training Loss: 21.317216981132077, Validation Loss: 37.486607142857146, Validation Accuracy: 76.44%\n",
            "Epoch 100/100, Training Loss: 20.673349056603772, Validation Loss: 38.09375, Validation Accuracy: 75.96%\n",
            "Test Accuracy: 76.89%\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td></td></tr><tr><td>test_accuracy</td><td></td></tr><tr><td>train_loss</td><td></td></tr><tr><td>val_accuracy</td><td></td></tr><tr><td>val_loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>100</td></tr><tr><td>test_accuracy</td><td>0.76887</td></tr><tr><td>train_loss</td><td>20.67335</td></tr><tr><td>val_accuracy</td><td>0.75962</td></tr><tr><td>val_loss</td><td>38.09375</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">1e-4</strong> at: <a href='https://wandb.ai/18750937507-uwa/snn_v5/runs/i1c9nkxb' target=\"_blank\">https://wandb.ai/18750937507-uwa/snn_v5/runs/i1c9nkxb</a><br/> View project at: <a href='https://wandb.ai/18750937507-uwa/snn_v5' target=\"_blank\">https://wandb.ai/18750937507-uwa/snn_v5</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240925_153030-i1c9nkxb/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "wandb.init(project=\"SNN_v5\", config={\n",
        "    \"learning_rate\": 1e-4,\n",
        "    \"epochs\": 100,\n",
        "    \"batch_size\": 32,\n",
        "}, name=\"1e-4\")\n",
        "\n",
        "# Access hyperparameters\n",
        "learning_rate = wandb.config.learning_rate\n",
        "n_epochs = wandb.config.epochs\n",
        "batch_size = wandb.config.batch_size\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "optimizer = Adam(net.s.parameters().astorch(), lr=learning_rate)\n",
        "loss_fun = MSELoss()\n",
        "n_epochs = 100\n",
        "set_seed(seed)\n",
        "\n",
        "# Function to calculate accuracy using target firing rate\n",
        "def calculate_accuracy_sum_gt(model, data_loader, target_firing_rate_value=10):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient calculations\n",
        "        for inputs, targets in data_loader:\n",
        "            inputs = inputs.transpose(1, 2)  # Ensure input shape matches model requirements\n",
        "            outputs, _, _ = model(inputs)\n",
        "            outputs = outputs.sum(dim=1)\n",
        "\n",
        "            # Create target firing rate tensor\n",
        "            target_firing_rate = torch.zeros_like(outputs).to(outputs.device)\n",
        "            target_firing_rate[targets == 1] = target_firing_rate_value\n",
        "            target_firing_rate[targets == 0] = 0\n",
        "\n",
        "            # Use sigmoid activation and round predictions for binary classification\n",
        "            predictions = torch.round(torch.sigmoid(outputs))\n",
        "\n",
        "            # Compare predictions with target firing rate (binary form)\n",
        "            binary_target = (target_firing_rate == target_firing_rate_value).float()\n",
        "\n",
        "            correct += (predictions == binary_target).sum().item()\n",
        "            total += targets.size(0)\n",
        "\n",
        "    accuracy = correct / total\n",
        "    return accuracy\n",
        "\n",
        "# Training and Validation loop\n",
        "for epoch in range(n_epochs):\n",
        "    net.s.train()  # Set the model to training mode\n",
        "    epoch_loss = 0.0  # Initialize running loss for training\n",
        "\n",
        "    # Training phase\n",
        "    for inputs, targets in train_loader:\n",
        "        optimizer.zero_grad()  # Clear gradients\n",
        "\n",
        "        # Adjust input dimensions to match model expectations\n",
        "        inputs = inputs.transpose(1, 2)\n",
        "\n",
        "        # Forward pass through the network\n",
        "        outputs, _, _ = net.s(inputs)  # Output shape [batch_size, n_time, n_classes]\n",
        "        outputs_sum = outputs.sum(dim=1)  # Sum spikes over the time dimension\n",
        "\n",
        "        # Create target firing rate tensor\n",
        "        target_firing_rate = torch.zeros_like(outputs_sum).to(outputs.device)\n",
        "        target_firing_rate[targets == 1] = 10  # Set firing rate for positive class\n",
        "        target_firing_rate[targets == 0] = 0   # Set firing rate for negative class\n",
        "\n",
        "        # Compute the loss using target firing rate\n",
        "        loss = loss_fun(outputs_sum, target_firing_rate)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumulate loss for this batch\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    # Calculate average training loss for the epoch\n",
        "    avg_train_loss = epoch_loss / len(train_loader)\n",
        "\n",
        "    # Validation phase\n",
        "    net.s.eval()  # Set the model to evaluation mode\n",
        "    val_loss = 0.0  # Initialize running loss for validation\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in val_loader:\n",
        "            inputs = inputs.transpose(1, 2)\n",
        "            outputs, _, _ = net.s(inputs)\n",
        "            outputs_sum = outputs.sum(dim=1)\n",
        "\n",
        "            # Create target firing rate tensor for validation\n",
        "            target_firing_rate = torch.zeros_like(outputs_sum).to(outputs.device)\n",
        "            target_firing_rate[targets == 1] = 10  # Firing rate for positive class\n",
        "            target_firing_rate[targets == 0] = 0   # Firing rate for negative class\n",
        "\n",
        "            # Compute validation loss\n",
        "            loss = loss_fun(outputs_sum, target_firing_rate)\n",
        "\n",
        "            # Accumulate validation loss\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    # Calculate average validation loss for the epoch\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "    # Calculate validation accuracy\n",
        "    val_accuracy = calculate_accuracy_sum_gt(net.s, val_loader)\n",
        "\n",
        "    # Log training loss, validation loss, and accuracy to W&B\n",
        "    wandb.log({\n",
        "        \"epoch\": epoch + 1,\n",
        "        \"train_loss\": avg_train_loss,\n",
        "        \"val_loss\": avg_val_loss,\n",
        "        \"val_accuracy\": val_accuracy\n",
        "    })\n",
        "\n",
        "    # Print training and validation losses, and validation accuracy for the current epoch\n",
        "    print(f'Epoch {epoch + 1}/{n_epochs}, Training Loss: {avg_train_loss}, Validation Loss: {avg_val_loss}, Validation Accuracy: {val_accuracy * 100:.2f}%')\n",
        "\n",
        "# Final evaluation on test data\n",
        "test_accuracy = calculate_accuracy_sum_gt(net.s, test_loader)\n",
        "\n",
        "# Log test accuracy to W&B\n",
        "wandb.log({\"test_accuracy\": test_accuracy})\n",
        "\n",
        "# Print the final test accuracy\n",
        "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n",
        "\n",
        "# Save the model\n",
        "torch.save(net.s.state_dict(), \"model_sum_gt_1e-4.pth\")\n",
        "wandb.save(\"model_sum_gt_1e.pth\")\n",
        "\n",
        "# Finish the W&B run\n",
        "wandb.finish()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "training and validation loss are similar through epochs, and validation accuracy drops and fluctuates\n",
        "test accuracy is higher than validation accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "893okgXHEAt3"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "trl2Cl_8zURu"
      },
      "outputs": [],
      "source": [
        "# Data augmentation\n",
        "# Time shift functionShifts the event data along the time axis.\n",
        "# Add noise functionAdds Gaussian noise to event data.\n",
        "# Random drop functionRandomly drops parts of the event data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "WHw08seNzS-q"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Data augmentation functions\n",
        "\n",
        "# Time shift functionShifts the event data along the time axis.\n",
        "def time_shift(event_data, max_shift=2):\n",
        "    shift = np.random.randint(-max_shift, max_shift)  # Randomly choose a shift amount\n",
        "    return np.roll(event_data, shift, axis=0)  # Apply shift along the time axis\n",
        "\n",
        "# Add noise functionAdds Gaussian noise to event data.\n",
        "def add_noise(event_data, noise_factor=0.001):\n",
        "    noise = np.random.normal(0, noise_factor, event_data.shape)  # poison noise\n",
        "    return event_data + noise  # Add noise to the data\n",
        "\n",
        "# Random drop functionRandomly drops parts of the event data.\n",
        "def random_drop(event_data, drop_rate=0.05):\n",
        "    mask = np.random.binomial(1, 1 - drop_rate, event_data.shape)  # Generate a mask to randomly drop parts of the data\n",
        "    return event_data * mask  # Apply the mask to the data\n",
        "\n",
        "augmentation_funcs = [time_shift, add_noise, random_drop]\n",
        "\n",
        "# Combine augmentation functions\n",
        "def augment_data(event_data, augmentation_funcs):\n",
        "    for func in augmentation_funcs:\n",
        "        event_data = func(event_data)  # Apply each augmentation function in sequence\n",
        "    return event_data\n",
        "\n",
        "# Data loader with augmentation applied to label 1 (event call) samples\n",
        "def augment_train_data(train_loader):\n",
        "    for batch_idx, (data, labels) in enumerate(train_loader):\n",
        "        for i in range(data.shape[0]):\n",
        "            if labels[i] == 1:  # Only augment data with label 1\n",
        "                augmented_data = augment_data(data[i].numpy(), augmentation_funcs)\n",
        "                data[i] = torch.from_numpy(augmented_data).to(data[i].device)\n",
        "        yield data, labels\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGjZIP-KOyXQ",
        "outputId": "ad3359a4-6dec-4077-b2d6-518c4de7358b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Labels distribution: tensor([836, 836])\n"
          ]
        }
      ],
      "source": [
        "train_loader_augmented=augment_train_data(train_loader)\n",
        "train_labels_augmented = torch.cat([batch[1] for batch in train_loader_augmented])\n",
        "print(f'Train Labels distribution: {torch.bincount(train_labels_augmented.int())}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.18.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/xiaoyuliu/Documents/school/capstone/project-12-prototype-bio-acoustic-detection-system-soundsentinel/wandb/run-20240919_230646-qi4t38sq</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/18750937507-uwa/SNN_meerkat/runs/qi4t38sq' target=\"_blank\">sum,gt,augment</a></strong> to <a href='https://wandb.ai/18750937507-uwa/SNN_meerkat' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/18750937507-uwa/SNN_meerkat' target=\"_blank\">https://wandb.ai/18750937507-uwa/SNN_meerkat</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/18750937507-uwa/SNN_meerkat/runs/qi4t38sq' target=\"_blank\">https://wandb.ai/18750937507-uwa/SNN_meerkat/runs/qi4t38sq</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10, Training Loss: 43.63030660377358, Validation Loss: 33.642857142857146, Validation Accuracy: 80.29%\n",
            "Epoch 2/10, Training Loss: 42.772995283018865, Validation Loss: 55.191964285714285, Validation Accuracy: 76.92%\n",
            "Epoch 3/10, Training Loss: 43.56367924528302, Validation Loss: 57.004464285714285, Validation Accuracy: 74.04%\n",
            "Epoch 4/10, Training Loss: 46.10495283018868, Validation Loss: 37.10267857142857, Validation Accuracy: 75.00%\n",
            "Epoch 5/10, Training Loss: 41.89858490566038, Validation Loss: 35.642857142857146, Validation Accuracy: 74.04%\n",
            "Epoch 6/10, Training Loss: 37.491745283018865, Validation Loss: 33.004464285714285, Validation Accuracy: 75.48%\n",
            "Epoch 7/10, Training Loss: 38.722877358490564, Validation Loss: 44.816964285714285, Validation Accuracy: 63.94%\n",
            "Epoch 8/10, Training Loss: 38.7435141509434, Validation Loss: 34.66517857142857, Validation Accuracy: 75.00%\n",
            "Epoch 9/10, Training Loss: 34.69516509433962, Validation Loss: 32.620535714285715, Validation Accuracy: 76.92%\n",
            "Epoch 10/10, Training Loss: 34.65094339622642, Validation Loss: 29.986607142857142, Validation Accuracy: 79.81%\n",
            "Test Accuracy: 82.08%\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td></td></tr><tr><td>test_accuracy</td><td></td></tr><tr><td>train_loss</td><td></td></tr><tr><td>val_accuracy</td><td></td></tr><tr><td>val_loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>test_accuracy</td><td>0.82075</td></tr><tr><td>train_loss</td><td>34.65094</td></tr><tr><td>val_accuracy</td><td>0.79808</td></tr><tr><td>val_loss</td><td>29.98661</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">sum,gt,augment</strong> at: <a href='https://wandb.ai/18750937507-uwa/SNN_meerkat/runs/qi4t38sq' target=\"_blank\">https://wandb.ai/18750937507-uwa/SNN_meerkat/runs/qi4t38sq</a><br/> View project at: <a href='https://wandb.ai/18750937507-uwa/SNN_meerkat' target=\"_blank\">https://wandb.ai/18750937507-uwa/SNN_meerkat</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240919_230646-qi4t38sq/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# outputs.sum(dim=1),target firing rate tensoraugment\n",
        "\n",
        "\n",
        "wandb.init(project=\"SNN_meerkat\", config={\n",
        "    \"learning_rate\": 1e-3,\n",
        "    \"epochs\": 10,\n",
        "    \"batch_size\": 32,\n",
        "}, name=\"sum,gt,augment\")\n",
        "\n",
        "# Access hyperparameters\n",
        "learning_rate = wandb.config.learning_rate\n",
        "n_epochs = wandb.config.epochs\n",
        "batch_size = wandb.config.batch_size\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "optimizer = Adam(net.parameters().astorch(), lr=learning_rate)\n",
        "loss_fun = MSELoss()\n",
        "n_epochs = 10\n",
        "set_seed(seed)\n",
        "\n",
        "\n",
        "\n",
        "# Training and Validation loop\n",
        "for epoch in range(n_epochs):\n",
        "    net.train()  # Set the model to training mode\n",
        "    epoch_loss = 0.0  # Initialize running loss for training\n",
        "\n",
        "    # Training phase\n",
        "    for inputs, targets in augment_train_data(train_loader):\n",
        "        optimizer.zero_grad()  # Clear gradients\n",
        "\n",
        "        # Adjust input dimensions to match model expectations\n",
        "        inputs = inputs.transpose(1, 2)\n",
        "\n",
        "        # Forward pass through the network\n",
        "        outputs, _, _ = net(inputs)  # Output shape [batch_size, n_time, n_classes]\n",
        "        outputs_sum = outputs.sum(dim=1)  # Sum spikes over the time dimension\n",
        "\n",
        "        # Create target firing rate tensor\n",
        "        target_firing_rate = torch.zeros_like(outputs_sum).to(outputs.device)\n",
        "        target_firing_rate[targets == 1] = 10  # Set firing rate for positive class\n",
        "        target_firing_rate[targets == 0] = 0   # Set firing rate for negative class\n",
        "\n",
        "        # Compute the loss using target firing rate\n",
        "        loss = loss_fun(outputs_sum, target_firing_rate)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumulate loss for this batch\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    # Calculate average training loss for the epoch\n",
        "    avg_train_loss = epoch_loss / len(train_loader)\n",
        "\n",
        "    # Validation phase\n",
        "    net.eval()  # Set the model to evaluation mode\n",
        "    val_loss = 0.0  # Initialize running loss for validation\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in val_loader:\n",
        "            inputs = inputs.transpose(1, 2)\n",
        "            outputs, _, _ = net(inputs)\n",
        "            outputs_sum = outputs.sum(dim=1)\n",
        "\n",
        "            # Create target firing rate tensor for validation\n",
        "            target_firing_rate = torch.zeros_like(outputs_sum).to(outputs.device)\n",
        "            target_firing_rate[targets == 1] = 10  # Firing rate for positive class\n",
        "            target_firing_rate[targets == 0] = 0   # Firing rate for negative class\n",
        "\n",
        "            # Compute validation loss\n",
        "            loss = loss_fun(outputs_sum, target_firing_rate)\n",
        "\n",
        "            # Accumulate validation loss\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    # Calculate average validation loss for the epoch\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "    # Calculate validation accuracy\n",
        "    val_accuracy = calculate_accuracy_sum_gt(net, val_loader)\n",
        "\n",
        "    # Log training loss, validation loss, and accuracy to W&B\n",
        "    wandb.log({\n",
        "        \"epoch\": epoch + 1,\n",
        "        \"train_loss\": avg_train_loss,\n",
        "        \"val_loss\": avg_val_loss,\n",
        "        \"val_accuracy\": val_accuracy\n",
        "    })\n",
        "\n",
        "    # Print training and validation losses, and validation accuracy for the current epoch\n",
        "    print(f'Epoch {epoch + 1}/{n_epochs}, Training Loss: {avg_train_loss}, Validation Loss: {avg_val_loss}, Validation Accuracy: {val_accuracy * 100:.2f}%')\n",
        "\n",
        "# Final evaluation on test data\n",
        "test_accuracy = calculate_accuracy_sum_gt(net, test_loader)\n",
        "\n",
        "# Log test accuracy to W&B\n",
        "wandb.log({\"test_accuracy\": test_accuracy})\n",
        "\n",
        "# Print the final test accuracy\n",
        "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n",
        "\n",
        "# Save the model\n",
        "torch.save(net.state_dict(), \"model_sum_gt_augment.pth\")\n",
        "wandb.save(\"model_sum_gt_augment.pth\")\n",
        "\n",
        "# Finish the W&B run\n",
        "wandb.finish()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# synnet with more hidden layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SynNet  with shape (16, 1) {\n",
            "    TorchSequential 'seq' with shape (16, 1) {\n",
            "        LinearTorch '0_LinearTorch' with shape (16, 32)\n",
            "        LIFTorch '1_LIFTorch' with shape (32, 32)\n",
            "        TimeStepDropout '2_TimeStepDropout' with shape (32,)\n",
            "        LinearTorch '3_LinearTorch' with shape (32, 32)\n",
            "        LIFTorch '4_LIFTorch' with shape (32, 32)\n",
            "        TimeStepDropout '5_TimeStepDropout' with shape (32,)\n",
            "        LinearTorch '6_LinearTorch' with shape (32, 32)\n",
            "        LIFTorch '7_LIFTorch' with shape (32, 32)\n",
            "        TimeStepDropout '8_TimeStepDropout' with shape (32,)\n",
            "        LinearTorch '9_LinearTorch' with shape (32, 1)\n",
            "        LIFTorch '10_LIFTorch' with shape (1, 1)\n",
            "    }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# Define dataset characteristics\n",
        "n_channels = 16  # Number of input channels\n",
        "n_classes = 1    # the output of MSELoss is discrite\n",
        "n_time = 101     # Number of time steps\n",
        "batch_size = 32  # Batch size\n",
        "\n",
        "# Initialize the SynNet model\n",
        "net_32 = SynNet(\n",
        "    p_dropout = 0.1,                        # Dropout proportion to use\n",
        "    n_channels=n_channels,                # Number of input channels\n",
        "    n_classes=n_classes,                  # Number of output classes (2 for binary classification)\n",
        "    size_hidden_layers=[32, 32, 32],      # Number of neurons in each hidden layer\n",
        "    time_constants_per_layer=[2, 4, 8],   # Time constants for each layer\n",
        ")\n",
        "print(net_32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.18.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/xiaoyuliu/Documents/school/capstone/project-12-prototype-bio-acoustic-detection-system-soundsentinel/wandb/run-20240919_231515-9tmdle16</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/18750937507-uwa/SNN_meerkat/runs/9tmdle16' target=\"_blank\">sum,gt,32layer</a></strong> to <a href='https://wandb.ai/18750937507-uwa/SNN_meerkat' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/18750937507-uwa/SNN_meerkat' target=\"_blank\">https://wandb.ai/18750937507-uwa/SNN_meerkat</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/18750937507-uwa/SNN_meerkat/runs/9tmdle16' target=\"_blank\">https://wandb.ai/18750937507-uwa/SNN_meerkat/runs/9tmdle16</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10, Training Loss: 64.84198113207547, Validation Loss: 162.03125, Validation Accuracy: 65.38%\n",
            "Epoch 2/10, Training Loss: 54.41391509433962, Validation Loss: 54.504464285714285, Validation Accuracy: 55.29%\n",
            "Epoch 3/10, Training Loss: 50.520636792452834, Validation Loss: 53.64732142857143, Validation Accuracy: 61.06%\n",
            "Epoch 4/10, Training Loss: 43.540683962264154, Validation Loss: 68.21875, Validation Accuracy: 70.67%\n",
            "Epoch 5/10, Training Loss: 41.97995283018868, Validation Loss: 52.02232142857143, Validation Accuracy: 64.90%\n",
            "Epoch 6/10, Training Loss: 40.787146226415096, Validation Loss: 46.20982142857143, Validation Accuracy: 67.79%\n",
            "Epoch 7/10, Training Loss: 40.295400943396224, Validation Loss: 44.4375, Validation Accuracy: 72.12%\n",
            "Epoch 8/10, Training Loss: 40.45754716981132, Validation Loss: 72.74553571428571, Validation Accuracy: 75.48%\n",
            "Epoch 9/10, Training Loss: 38.29716981132076, Validation Loss: 64.05357142857143, Validation Accuracy: 72.60%\n",
            "Epoch 10/10, Training Loss: 37.12735849056604, Validation Loss: 43.424107142857146, Validation Accuracy: 73.08%\n",
            "Test Accuracy: 76.42%\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td></td></tr><tr><td>test_accuracy</td><td></td></tr><tr><td>train_loss</td><td></td></tr><tr><td>val_accuracy</td><td></td></tr><tr><td>val_loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>test_accuracy</td><td>0.76415</td></tr><tr><td>train_loss</td><td>37.12736</td></tr><tr><td>val_accuracy</td><td>0.73077</td></tr><tr><td>val_loss</td><td>43.42411</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">sum,gt,32layer</strong> at: <a href='https://wandb.ai/18750937507-uwa/SNN_meerkat/runs/9tmdle16' target=\"_blank\">https://wandb.ai/18750937507-uwa/SNN_meerkat/runs/9tmdle16</a><br/> View project at: <a href='https://wandb.ai/18750937507-uwa/SNN_meerkat' target=\"_blank\">https://wandb.ai/18750937507-uwa/SNN_meerkat</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240919_231515-9tmdle16/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "wandb.init(project=\"SNN_meerkat\", config={\n",
        "    \"learning_rate\": 1e-3,\n",
        "    \"epochs\": 10,\n",
        "    \"batch_size\": 32,\n",
        "},name=\"sum,gt,32layer\")\n",
        "\n",
        "# Access hyperparameters\n",
        "learning_rate = wandb.config.learning_rate\n",
        "n_epochs = wandb.config.epochs\n",
        "batch_size = wandb.config.batch_size\n",
        "\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "optimizer = Adam(net_32.parameters().astorch(), lr=1e-3)\n",
        "loss_fun = MSELoss()\n",
        "n_epochs = 30\n",
        "set_seed(seed)\n",
        "\n",
        "# Function to calculate accuracy using target firing rate\n",
        "def calculate_accuracy_sum_gt(model, data_loader, target_firing_rate_value=10):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient calculations\n",
        "        for inputs, targets in data_loader:\n",
        "            inputs = inputs.transpose(1, 2)  # Ensure input shape matches model requirements\n",
        "            outputs, _, _ = model(inputs)\n",
        "            outputs = outputs.sum(dim=1)\n",
        "\n",
        "            # Create target firing rate tensor\n",
        "            target_firing_rate = torch.zeros_like(outputs).to(outputs.device)\n",
        "            target_firing_rate[targets == 1] = target_firing_rate_value\n",
        "            target_firing_rate[targets == 0] = 0\n",
        "\n",
        "            # Use sigmoid activation and round predictions for binary classification\n",
        "            predictions = torch.round(torch.sigmoid(outputs))\n",
        "\n",
        "            # Compare predictions with target firing rate (binary form)\n",
        "            binary_target = (target_firing_rate == target_firing_rate_value).float()\n",
        "\n",
        "            correct += (predictions == binary_target).sum().item()\n",
        "            total += targets.size(0)\n",
        "\n",
        "    accuracy = correct / total\n",
        "    return accuracy\n",
        "\n",
        "# Training and Validation loop\n",
        "for epoch in range(n_epochs):\n",
        "    net_32.train()  # Set the model to training mode\n",
        "    epoch_loss = 0.0  # Initialize running loss for training\n",
        "\n",
        "    # Training phase\n",
        "    for inputs, targets in train_loader:\n",
        "        optimizer.zero_grad()  # Clear gradients\n",
        "\n",
        "        # Adjust input dimensions to match model expectations\n",
        "        inputs = inputs.transpose(1, 2)\n",
        "\n",
        "        # Forward pass through the network\n",
        "        outputs, _, _ = net_32(inputs)  # Output shape [batch_size, n_time, n_classes]\n",
        "        outputs_sum = outputs.sum(dim=1)  # Sum spikes over the time dimension\n",
        "\n",
        "        # Create target firing rate tensor\n",
        "        target_firing_rate = torch.zeros_like(outputs_sum).to(outputs.device)\n",
        "        target_firing_rate[targets == 1] = 10  # Set firing rate for positive class\n",
        "        target_firing_rate[targets == 0] = 0   # Set firing rate for negative class\n",
        "\n",
        "        # Compute the loss using target firing rate\n",
        "        loss = loss_fun(outputs_sum, target_firing_rate)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumulate loss for this batch\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    # Calculate average training loss for the epoch\n",
        "    avg_train_loss = epoch_loss / len(train_loader)\n",
        "\n",
        "    # Validation phase\n",
        "    net_32.eval()  # Set the model to evaluation mode\n",
        "    val_loss = 0.0  # Initialize running loss for validation\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in val_loader:\n",
        "            inputs = inputs.transpose(1, 2)\n",
        "            outputs, _, _ = net_32(inputs)\n",
        "            outputs_sum = outputs.sum(dim=1)\n",
        "\n",
        "            # Create target firing rate tensor for validation\n",
        "            target_firing_rate = torch.zeros_like(outputs_sum).to(outputs.device)\n",
        "            target_firing_rate[targets == 1] = 10  # Firing rate for positive class\n",
        "            target_firing_rate[targets == 0] = 0   # Firing rate for negative class\n",
        "\n",
        "            # Compute validation loss\n",
        "            loss = loss_fun(outputs_sum, target_firing_rate)\n",
        "\n",
        "            # Accumulate validation loss\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    # Calculate average validation loss for the epoch\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "    # Calculate validation accuracy\n",
        "    val_accuracy = calculate_accuracy_sum_gt(net_32, val_loader)\n",
        "\n",
        "    # Log training loss, validation loss, and accuracy to W&B\n",
        "    wandb.log({\n",
        "        \"epoch\": epoch + 1,\n",
        "        \"train_loss\": avg_train_loss,\n",
        "        \"val_loss\": avg_val_loss,\n",
        "        \"val_accuracy\": val_accuracy\n",
        "    })\n",
        "\n",
        "    # Print training and validation losses, and validation accuracy for the current epoch\n",
        "    print(f'Epoch {epoch + 1}/{n_epochs}, Training Loss: {avg_train_loss}, Validation Loss: {avg_val_loss}, Validation Accuracy: {val_accuracy * 100:.2f}%')\n",
        "\n",
        "# Final evaluation on test data\n",
        "test_accuracy = calculate_accuracy_sum_gt(net_32, test_loader)\n",
        "\n",
        "# Log test accuracy to W&B\n",
        "wandb.log({\"test_accuracy\": test_accuracy})\n",
        "\n",
        "# Print the final test accuracy\n",
        "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n",
        "\n",
        "# Save the model\n",
        "torch.save(net_32.state_dict(), \"model_sum_32.pth\")\n",
        "wandb.save(\"model_sum_32.pth\")\n",
        "\n",
        "# Finish the W&B run\n",
        "wandb.finish()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.18.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/xiaoyuliu/Documents/school/capstone/project-12-prototype-bio-acoustic-detection-system-soundsentinel/wandb/run-20240919_231724-gtq946vp</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/18750937507-uwa/SNN_meerkat/runs/gtq946vp' target=\"_blank\">sum,gt,32layer,augment</a></strong> to <a href='https://wandb.ai/18750937507-uwa/SNN_meerkat' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/18750937507-uwa/SNN_meerkat' target=\"_blank\">https://wandb.ai/18750937507-uwa/SNN_meerkat</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/18750937507-uwa/SNN_meerkat/runs/gtq946vp' target=\"_blank\">https://wandb.ai/18750937507-uwa/SNN_meerkat/runs/gtq946vp</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10, Training Loss: 51.545400943396224, Validation Loss: 43.142857142857146, Validation Accuracy: 75.00%\n",
            "Epoch 2/10, Training Loss: 56.68514150943396, Validation Loss: 53.245535714285715, Validation Accuracy: 49.04%\n",
            "Epoch 3/10, Training Loss: 59.549528301886795, Validation Loss: 49.330357142857146, Validation Accuracy: 55.29%\n",
            "Epoch 4/10, Training Loss: 49.51002358490566, Validation Loss: 48.99107142857143, Validation Accuracy: 63.46%\n",
            "Epoch 5/10, Training Loss: 48.696933962264154, Validation Loss: 47.933035714285715, Validation Accuracy: 65.38%\n",
            "Epoch 6/10, Training Loss: 45.73997641509434, Validation Loss: 56.473214285714285, Validation Accuracy: 64.90%\n",
            "Epoch 7/10, Training Loss: 57.9622641509434, Validation Loss: 48.84375, Validation Accuracy: 55.77%\n",
            "Epoch 8/10, Training Loss: 45.308372641509436, Validation Loss: 50.92857142857143, Validation Accuracy: 60.10%\n",
            "Epoch 9/10, Training Loss: 48.21521226415094, Validation Loss: 46.5625, Validation Accuracy: 63.94%\n",
            "Epoch 10/10, Training Loss: 45.959316037735846, Validation Loss: 51.575892857142854, Validation Accuracy: 53.85%\n",
            "Test Accuracy: 55.66%\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td></td></tr><tr><td>test_accuracy</td><td></td></tr><tr><td>train_loss</td><td></td></tr><tr><td>val_accuracy</td><td></td></tr><tr><td>val_loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>test_accuracy</td><td>0.5566</td></tr><tr><td>train_loss</td><td>45.95932</td></tr><tr><td>val_accuracy</td><td>0.53846</td></tr><tr><td>val_loss</td><td>51.57589</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">sum,gt,32layer,augment</strong> at: <a href='https://wandb.ai/18750937507-uwa/SNN_meerkat/runs/gtq946vp' target=\"_blank\">https://wandb.ai/18750937507-uwa/SNN_meerkat/runs/gtq946vp</a><br/> View project at: <a href='https://wandb.ai/18750937507-uwa/SNN_meerkat' target=\"_blank\">https://wandb.ai/18750937507-uwa/SNN_meerkat</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240919_231724-gtq946vp/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "wandb.init(project=\"SNN_meerkat\", config={\n",
        "    \"learning_rate\": 1e-3,\n",
        "    \"epochs\": 10,\n",
        "    \"batch_size\": 32,\n",
        "},name=\"sum,gt,32layer,augment\")\n",
        "\n",
        "# Access hyperparameters\n",
        "learning_rate = wandb.config.learning_rate\n",
        "n_epochs = wandb.config.epochs\n",
        "batch_size = wandb.config.batch_size\n",
        "\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "optimizer = Adam(net_32.parameters().astorch(), lr=1e-3)\n",
        "loss_fun = MSELoss()\n",
        "n_epochs = 10\n",
        "set_seed(seed)\n",
        "\n",
        "\n",
        "\n",
        "# Training and Validation loop\n",
        "for epoch in range(n_epochs):\n",
        "    net_32.train()  # Set the model to training mode\n",
        "    epoch_loss = 0.0  # Initialize running loss for training\n",
        "\n",
        "    # Training phase\n",
        "    for inputs, targets in augment_train_data(train_loader):\n",
        "        optimizer.zero_grad()  # Clear gradients\n",
        "\n",
        "        # Adjust input dimensions to match model expectations\n",
        "        inputs = inputs.transpose(1, 2)\n",
        "\n",
        "        # Forward pass through the network\n",
        "        outputs, _, _ = net_32(inputs)  # Output shape [batch_size, n_time, n_classes]\n",
        "        outputs_sum = outputs.sum(dim=1)  # Sum spikes over the time dimension\n",
        "\n",
        "        # Create target firing rate tensor\n",
        "        target_firing_rate = torch.zeros_like(outputs_sum).to(outputs.device)\n",
        "        target_firing_rate[targets == 1] = 10  # Set firing rate for positive class\n",
        "        target_firing_rate[targets == 0] = 0   # Set firing rate for negative class\n",
        "\n",
        "        # Compute the loss using target firing rate\n",
        "        loss = loss_fun(outputs_sum, target_firing_rate)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumulate loss for this batch\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    # Calculate average training loss for the epoch\n",
        "    avg_train_loss = epoch_loss / len(train_loader)\n",
        "\n",
        "    # Validation phase\n",
        "    net_32.eval()  # Set the model to evaluation mode\n",
        "    val_loss = 0.0  # Initialize running loss for validation\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in val_loader:\n",
        "            inputs = inputs.transpose(1, 2)\n",
        "            outputs, _, _ = net_32(inputs)\n",
        "            outputs_sum = outputs.sum(dim=1)\n",
        "\n",
        "            # Create target firing rate tensor for validation\n",
        "            target_firing_rate = torch.zeros_like(outputs_sum).to(outputs.device)\n",
        "            target_firing_rate[targets == 1] = 10  # Firing rate for positive class\n",
        "            target_firing_rate[targets == 0] = 0   # Firing rate for negative class\n",
        "\n",
        "            # Compute validation loss\n",
        "            loss = loss_fun(outputs_sum, target_firing_rate)\n",
        "\n",
        "            # Accumulate validation loss\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    # Calculate average validation loss for the epoch\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "    # Calculate validation accuracy\n",
        "    val_accuracy = calculate_accuracy_sum_gt(net_32, val_loader)\n",
        "\n",
        "    # Log training loss, validation loss, and accuracy to W&B\n",
        "    wandb.log({\n",
        "        \"epoch\": epoch + 1,\n",
        "        \"train_loss\": avg_train_loss,\n",
        "        \"val_loss\": avg_val_loss,\n",
        "        \"val_accuracy\": val_accuracy\n",
        "    })\n",
        "\n",
        "    # Print training and validation losses, and validation accuracy for the current epoch\n",
        "    print(f'Epoch {epoch + 1}/{n_epochs}, Training Loss: {avg_train_loss}, Validation Loss: {avg_val_loss}, Validation Accuracy: {val_accuracy * 100:.2f}%')\n",
        "\n",
        "# Final evaluation on test data\n",
        "test_accuracy = calculate_accuracy_sum_gt(net_32, test_loader)\n",
        "\n",
        "# Log test accuracy to W&B\n",
        "wandb.log({\"test_accuracy\": test_accuracy})\n",
        "\n",
        "# Print the final test accuracy\n",
        "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n",
        "\n",
        "# Save the model\n",
        "torch.save(net_32.state_dict(), \"model_sum_32.pth\")\n",
        "wandb.save(\"model_sum_32.pth\")\n",
        "\n",
        "# Finish the W&B run\n",
        "wandb.finish()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.18.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/xiaoyuliu/Documents/school/capstone/project-12-prototype-bio-acoustic-detection-system-soundsentinel/wandb/run-20240917_150018-vohnr0f7</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/18750937507-uwa/SNN_meerkat/runs/vohnr0f7' target=\"_blank\">32layer, augmented_data, outputs.mean,learning_rate=1e-4</a></strong> to <a href='https://wandb.ai/18750937507-uwa/SNN_meerkat' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/18750937507-uwa/SNN_meerkat' target=\"_blank\">https://wandb.ai/18750937507-uwa/SNN_meerkat</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/18750937507-uwa/SNN_meerkat/runs/vohnr0f7' target=\"_blank\">https://wandb.ai/18750937507-uwa/SNN_meerkat/runs/vohnr0f7</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10, Training Loss: 0.1021600252474254, Validation Loss: 0.26871928333171774, Validation Accuracy: 84.13%\n",
            "Epoch 2/10, Training Loss: 0.09652927820131464, Validation Loss: 0.25976357316332205, Validation Accuracy: 81.73%\n",
            "Epoch 3/10, Training Loss: 0.09373572356296035, Validation Loss: 0.26413463934191633, Validation Accuracy: 83.17%\n",
            "Epoch 4/10, Training Loss: 0.09605242301411224, Validation Loss: 0.2624313802059208, Validation Accuracy: 80.29%\n",
            "Epoch 5/10, Training Loss: 0.1035680463713295, Validation Loss: 0.2693499071257455, Validation Accuracy: 80.29%\n",
            "Epoch 6/10, Training Loss: 0.09407506890173228, Validation Loss: 0.2604817295713084, Validation Accuracy: 75.00%\n",
            "Epoch 7/10, Training Loss: 0.09762089910372249, Validation Loss: 0.30404669924506117, Validation Accuracy: 79.33%\n",
            "Epoch 8/10, Training Loss: 0.09325886995724912, Validation Loss: 0.2740780668599265, Validation Accuracy: 80.29%\n",
            "Epoch 9/10, Training Loss: 0.1008986473645804, Validation Loss: 0.2699350163872753, Validation Accuracy: 79.81%\n",
            "Epoch 10/10, Training Loss: 0.08856957367146914, Validation Loss: 0.25909399959657875, Validation Accuracy: 71.63%\n",
            "Training completed\n",
            "Test Accuracy: 76.42%\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td></td></tr><tr><td>test_accuracy</td><td></td></tr><tr><td>train_loss</td><td></td></tr><tr><td>val_accuracy</td><td></td></tr><tr><td>val_loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>test_accuracy</td><td>0.76415</td></tr><tr><td>train_loss</td><td>0.08857</td></tr><tr><td>val_accuracy</td><td>0.71635</td></tr><tr><td>val_loss</td><td>0.25909</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">32layer, augmented_data, outputs.mean,learning_rate=1e-4</strong> at: <a href='https://wandb.ai/18750937507-uwa/SNN_meerkat/runs/vohnr0f7' target=\"_blank\">https://wandb.ai/18750937507-uwa/SNN_meerkat/runs/vohnr0f7</a><br/> View project at: <a href='https://wandb.ai/18750937507-uwa/SNN_meerkat' target=\"_blank\">https://wandb.ai/18750937507-uwa/SNN_meerkat</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240917_150018-vohnr0f7/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "wandb.init(project=\"SNN_meerkat\", config={\n",
        "    \"learning_rate\": 1e-4,\n",
        "    \"epochs\": 10,\n",
        "    \"batch_size\": 32,\n",
        "},name=\"32layer, augmented_data, outputs.mean,learning_rate=1e-4\")\n",
        "\n",
        "# Access hyperparameters\n",
        "learning_rate = wandb.config.learning_rate\n",
        "n_epochs = wandb.config.epochs\n",
        "batch_size = wandb.config.batch_size\n",
        "\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "optimizer = Adam(net_32.parameters().astorch(), lr=1e-4)\n",
        "loss_fun = MSELoss()\n",
        "n_epochs = 10\n",
        "\n",
        "def calculate_accuracy(model, data_loader):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient calculations\n",
        "        for inputs, targets in data_loader:\n",
        "            inputs = inputs.transpose(1, 2)  # Ensure input shape matches model requirements\n",
        "            outputs, _, _ = model(inputs)\n",
        "            outputs = outputs.mean(dim=1)\n",
        "\n",
        "            predictions = torch.round(torch.sigmoid(outputs))  # For binary classification\n",
        "            correct += (predictions == targets.unsqueeze(1)).sum().item()\n",
        "            total += targets.size(0)\n",
        "\n",
        "    accuracy = correct / total\n",
        "    return accuracy\n",
        "\n",
        "# Training and Validation loop\n",
        "for epoch in range(n_epochs):  # Progress bar for epochs\n",
        "    net_32.train()  # Set the model to training mode\n",
        "    epoch_loss = 0.0  # Initialize running loss for training\n",
        "\n",
        "    # Training phase\n",
        "    for inputs, targets in augment_train_data(train_loader): # Use augmented data\n",
        "\n",
        "        optimizer.zero_grad()  # Clear gradients\n",
        "\n",
        "        # Adjust input dimensions to match model expectations\n",
        "        inputs = inputs.transpose(1, 2)  # [batch_size, n_time, n_channels]\n",
        "\n",
        "        # Forward pass through the network\n",
        "        output, _, _ = net_32(inputs)  # Output shape [batch_size, n_time, n_classes]\n",
        "        output = output.mean(dim=1)  # mean spikes over the time dimension [batch_size, n_classes]\n",
        "\n",
        "        # Ensure targets are of float type as required by MSELoss\n",
        "        targets = targets.float().unsqueeze(1)  # Reshape targets to [batch_size, 1]\n",
        "\n",
        "        # Compute the loss\n",
        "        loss = loss_fun(output, targets)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()  # Compute gradients\n",
        "        optimizer.step()  # Update model parameters\n",
        "\n",
        "        # Accumulate loss for this batch\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    # Calculate average training loss for the epoch\n",
        "    avg_train_loss = epoch_loss / len(train_loader)\n",
        "\n",
        "    # Validation phase\n",
        "    net_32.eval()  # Set the model to evaluation mode\n",
        "    val_loss = 0.0  # Initialize running loss for validation\n",
        "    with torch.no_grad():  # Disable gradient calculation for validation\n",
        "        for inputs, targets in val_loader:\n",
        "            # Adjust input dimensions to match model expectations\n",
        "            inputs = inputs.transpose(1, 2)  # [batch_size, n_time, n_channels]\n",
        "\n",
        "            # Forward pass through the network\n",
        "            output, _, _ = net_32(inputs)\n",
        "            output = output.mean(dim=1)  # mean spikes over the time dimension\n",
        "\n",
        "            # Ensure targets are of float type\n",
        "            targets = targets.float().unsqueeze(1)\n",
        "\n",
        "            # Compute validation loss\n",
        "            loss = loss_fun(output, targets)\n",
        "\n",
        "            # Accumulate validation loss\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    # Calculate average validation loss for the epoch\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "    # Calculate validation accuracy\n",
        "    val_accuracy = calculate_accuracy(net_32, val_loader)\n",
        "\n",
        "    # Log training loss, validation loss, and accuracy to W&B\n",
        "    wandb.log({\n",
        "        \"epoch\": epoch + 1,\n",
        "        \"train_loss\": avg_train_loss,\n",
        "        \"val_loss\": avg_val_loss,\n",
        "        \"val_accuracy\": val_accuracy  # Log accuracy\n",
        "    })\n",
        "\n",
        "    # Print training and validation losses, and validation accuracy for the current epoch\n",
        "    print(f'Epoch {epoch + 1}/{n_epochs}, Training Loss: {avg_train_loss}, Validation Loss: {avg_val_loss}, Validation Accuracy: {val_accuracy * 100:.2f}%')\n",
        "\n",
        "# End of training\n",
        "print('Training completed')\n",
        "\n",
        "\n",
        "\n",
        "# Final evaluation on test data (to evaluate generalization after training)\n",
        "test_accuracy = calculate_accuracy(net_32, test_loader)\n",
        "\n",
        "# Log test accuracy to W&B\n",
        "wandb.log({\"test_accuracy\": test_accuracy})\n",
        "\n",
        "# Print the final test accuracy\n",
        "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n",
        "\n",
        "\n",
        "# Optionally save the model using W&B\n",
        "torch.save(net_32.state_dict(), \"model_32layer_augmented_1e-4.pth\")\n",
        "wandb.save(\"model_32layer_augmented_1e-4.pth\")\n",
        "\n",
        "# Finish the W&B run\n",
        "wandb.finish()\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
